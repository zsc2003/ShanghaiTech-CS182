{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixtures of Gaussians and Expectation Maximization algorithm\n",
    "\n",
    "\n",
    "The following manuscript from PRML derives the EM algorithm for Mixtures of Gaussians. You can refer to PRML 9.2 for more details.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the Gaussian mixture distribution can be written as a linear superposition of Gaussians,\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x})=\\sum_{k=1}^K \\pi_k \\mathcal{N}\\left(\\mathbf{x} \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k\\right) .\n",
    "$$\n",
    "\n",
    "Let us introduce a $K$-dimensional binary random variable $\\mathrm{z}$ having a 1 -of- $K$ representation in which a particular element $z_k$ is equal to 1 and all other elements are equal to 0 . The values of $z_k$ therefore satisfy $z_k \\in\\{0,1\\}$ and $\\sum_k z_k=1$, and we see that there are $K$ possible states for the vector $\\mathbf{z}$ according to which element is nonzero. We shall define the joint distribution $p(\\mathbf{x}, \\mathbf{z})$ in terms of a marginal distribution $p(\\mathbf{z})$ and a conditional distribution $p(\\mathbf{x} \\mid \\mathbf{z})$. The marginal distribution over $\\mathbf{z}$ is specified in terms of the mixing coefficients $\\pi_k$, such that\n",
    "$$\n",
    "p\\left(z_k=1\\right)=\\pi_k\n",
    "$$\n",
    "where the parameters $\\left\\{\\pi_k\\right\\}$ must satisfy\n",
    "$$\n",
    "0 \\leqslant \\pi_k \\leqslant 1\n",
    "$$\n",
    "together with\n",
    "$$\n",
    "\\sum_{k=1}^K \\pi_k=1\n",
    "$$\n",
    "in order to be valid probabilities. Because $\\mathbf{z}$ uses a 1 -of- $K$ representation, we can also write this distribution in the form\n",
    "$$\n",
    "p(\\mathbf{z})=\\prod_{k=1}^K \\pi_k^{z_k} .\n",
    "$$\n",
    "\n",
    "Similarly, the conditional distribution of $\\mathbf{x}$ given a particular value for $\\mathbf{z}$ is a Gaussian\n",
    "$$\n",
    "p\\left(\\mathbf{x} \\mid z_k=1\\right)=\\mathcal{N}\\left(\\mathbf{x} \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k\\right)\n",
    "$$\n",
    "which can also be written in the form\n",
    "$$\n",
    "p(\\mathbf{x} \\mid \\mathbf{z})=\\prod_{k=1}^K \\mathcal{N}\\left(\\mathbf{x} \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k\\right)^{z_k} .\n",
    "$$\n",
    "\n",
    "The joint distribution is given by $p(\\mathbf{z}) p(\\mathbf{x} \\mid \\mathbf{z})$, and the marginal distribution of $\\mathbf{x}$ is then obtained by summing the joint distribution over all possible states of $\\mathbf{z}$ to give\n",
    "$$\n",
    "p(\\mathbf{x})=\\sum_{\\mathbf{z}} p(\\mathbf{z}) p(\\mathbf{x} \\mid \\mathbf{z})=\\sum_{k=1}^K \\pi_k \\mathcal{N}\\left(\\mathbf{x} \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k\\right).\n",
    "$$\n",
    "Thus the marginal distribution of $\\mathbf{x}$ is a Gaussian mixture of the form (9.7). If we have several observations $\\mathbf{x}_1, \\ldots, \\mathbf{x}_N$, then, because we have represented the marginal distribution in the form $p(\\mathbf{x})=$ $\\sum_{\\mathbf{z}} p(\\mathbf{x}, \\mathbf{z})$, it follows that for every observed data point $\\mathbf{x}_n$ there is a corresponding latent variable $\\mathbf{z}_n$.\n",
    "\n",
    "\n",
    "We shall use $\\gamma\\left(z_k\\right)$ to denote $p\\left(z_k=1 \\mid \\mathbf{x}\\right)$, whose value can be found using Bayes' theorem\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\gamma\\left(z_k\\right) \\equiv p\\left(z_k=1 \\mid \\mathbf{x}\\right) & =\\frac{p\\left(z_k=1\\right) p\\left(\\mathbf{x} \\mid z_k=1\\right)}{\\sum_{j=1}^K p\\left(z_j=1\\right) p\\left(\\mathbf{x} \\mid z_j=1\\right)} \\\\\n",
    "& =\\frac{\\pi_k \\mathcal{N}\\left(\\mathbf{x} \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k\\right)}{\\sum_{j=1}^K \\pi_j \\mathcal{N}\\left(\\mathbf{x} \\mid \\boldsymbol{\\mu}_j, \\boldsymbol{\\Sigma}_j\\right)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log of the likelihood function is given by\n",
    "$$\n",
    "\\log p(\\mathbf{X}) = \\sum_{n=1}^N \\log \\left\\{ \\sum^{K}_{k=1} \\pi_k \\mathcal{N}(\\mathbf{x}_n|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) \\right\\}.\n",
    "$$\n",
    "Let us begin by writing down the conditions that must be satisfied at a maximum of the likelihood function. Setting the derivatives of $\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$ with respect to the means $\\boldsymbol{\\mu}_k$ of the Gaussian components to zero, we obtain\n",
    "$$\n",
    "0=-\\sum_{n=1}^N \\underbrace{\\frac{\\pi_k \\mathcal{N}\\left(\\mathbf{x}_n \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k\\right)}{\\sum_j \\pi_j \\mathcal{N}\\left(\\mathbf{x}_n \\mid \\boldsymbol{\\mu}_j, \\boldsymbol{\\Sigma}_j\\right)}}_{\\gamma\\left(z_{n k}\\right)} \\boldsymbol{\\Sigma}_k\\left(\\mathbf{x}_n-\\boldsymbol{\\mu}_k\\right)\n",
    "$$\n",
    "where we have made use of the form (2.43) for the Gaussian distribution. Note that the posterior probabilities, or responsibilities, given by (9.13) appear naturally on the right-hand side. Multiplying by $\\boldsymbol{\\Sigma}_k^{-1}$ (which we assume to be nonsingular) and rearranging we obtain\n",
    "$$\n",
    "\\boldsymbol{\\mu}_k=\\frac{1}{N_k} \\sum_{n=1}^N \\gamma\\left(z_{n k}\\right) \\mathbf{x}_n\n",
    "$$\n",
    "where we have defined\n",
    "$$\n",
    "N_k=\\sum_{n=1}^N \\gamma\\left(z_{n k}\\right)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we set the derivative of $\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$ with respect to $\\boldsymbol{\\Sigma}_k$ to zero, and follow a similar line of reasoning, making use of the result for the maximum likelihood solution for the covariance matrix of a single Gaussian, we obtain\n",
    "$$\n",
    "\\boldsymbol{\\Sigma}_k=\\frac{1}{N_k} \\sum_{n=1}^N \\gamma\\left(z_{n k}\\right)\\left(\\mathbf{x}_n-\\boldsymbol{\\mu}_k\\right)\\left(\\mathbf{x}_n-\\boldsymbol{\\mu}_k\\right)^{\\mathrm{T}}\n",
    "$$\n",
    "which has the same form as the corresponding result for a single Gaussian fitted to the data set, but again with each data point weighted by the corresponding posterior probability and with the denominator given by the effective number of points associated with the corresponding component.\n",
    "\n",
    "Finally, we maximize $\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$ with respect to the mixing coefficients $\\pi_k$. Here we must take account of the constraint (9.9), which requires the mixing coefficients to sum to one. This can be achieved using a Lagrange multiplier and maximizing the following quantity\n",
    "$$\n",
    "\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})+\\lambda\\left(\\sum_{k=1}^K \\pi_k-1\\right)\n",
    "$$\n",
    "which gives\n",
    "$$\n",
    "0=\\sum_{n=1}^N \\frac{\\mathcal{N}\\left(\\mathbf{x}_n \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k\\right)}{\\sum_j \\pi_j \\mathcal{N}\\left(\\mathbf{x}_n \\mid \\boldsymbol{\\mu}_j, \\boldsymbol{\\Sigma}_j\\right)}+\\lambda\n",
    "$$\n",
    "where again we see the appearance of the responsibilities. If we now multiply both sides by $\\pi_k$ and sum over $k$ making use of the constraint (9.9), we find $\\lambda=-N$. Using this to eliminate $\\lambda$ and rearranging we obtain\n",
    "$$\n",
    "\\pi_k=\\frac{N_k}{N}\n",
    "$$\n",
    "so that the mixing coefficient for the $k^{\\text {th }}$ component is given by the average responsibility which that component takes for explaining the data points."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM for Gaussian Mixtures\n",
    "Given a Gaussian mixture model, the goal is to maximize the likelihood function with respect to the parameters (comprising the means and covariances of the components and the mixing coefficients).\n",
    "1. Initialize the means $\\boldsymbol{\\mu}_k$, covariances $\\boldsymbol{\\Sigma}_k$ and mixing coefficients $\\pi_k$, and evaluate the initial value of the log likelihood.\n",
    "2. E step. Evaluate the responsibilities using the current parameter values\n",
    "$$\n",
    "\\gamma\\left(z_{n k}\\right)=\\frac{\\pi_k \\mathcal{N}\\left(\\mathbf{x}_n \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k\\right)}{\\sum_{j=1}^K \\pi_j \\mathcal{N}\\left(\\mathbf{x}_n \\mid \\boldsymbol{\\mu}_j, \\boldsymbol{\\Sigma}_j\\right)} .\n",
    "$$\n",
    "3. M step. Re-estimate the parameters using the current responsibilities\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{\\mu}_k^{\\text {new }} & =\\frac{1}{N_k} \\sum_{n=1}^N \\gamma\\left(z_{n k}\\right) \\mathbf{x}_n \\\\\n",
    "\\boldsymbol{\\Sigma}_k^{\\text {new }} & =\\frac{1}{N_k} \\sum_{n=1}^N \\gamma\\left(z_{n k}\\right)\\left(\\mathbf{x}_n-\\boldsymbol{\\mu}_k^{\\text {new }}\\right)\\left(\\mathbf{x}_n-\\boldsymbol{\\mu}_k^{\\text {new }}\\right)^{\\mathrm{T}} \\\\\n",
    "\\pi_k^{\\text {new }} & =\\frac{N_k}{N}\n",
    "\\end{aligned}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "N_k=\\sum_{n=1}^N \\gamma\\left(z_{n k}\\right) .\n",
    "$$\n",
    "4. Evaluate the log likelihood\n",
    "$$\n",
    "\\ln p(\\mathbf{X} \\mid \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}, \\boldsymbol{\\pi})=\\sum_{n=1}^N \\ln \\left\\{\\sum_{k=1}^K \\pi_k \\mathcal{N}\\left(\\mathbf{x}_n \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k\\right)\\right\\}\n",
    "$$\n",
    "and check for convergence of either the parameters or the log likelihood. If the convergence criterion is not satisfied return to step 2 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install scipy\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# The seed is fixed for reproducibility.\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(X, pi, mu, sigma):\n",
    "    n, d = X.shape\n",
    "    k = len(pi)\n",
    "    ll = 0\n",
    "\n",
    "    ################################################################################\n",
    "    # TODO:                                                                        #\n",
    "    # Calculate the log-likelihood (while this is not essential for vanilla EM)    #\n",
    "    # Hint:  try use multivariate_normal.                                          #\n",
    "    ################################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw color points according to their cluster using pyplot, you should not edit this function and can skip it safely.\n",
    "def draw(X, pi, mu, sigma, iter):\n",
    "    n, d = X.shape\n",
    "    k = len(mu)\n",
    "    gamma = np.zeros((n, k))\n",
    "    for i in range(n):\n",
    "        for j in range(k):\n",
    "            gamma[i, j] = pi[j] * multivariate_normal(mean=mu[j], cov=sigma[j]).pdf(X[i])\n",
    "        gamma[i] /= np.sum(gamma[i])\n",
    "    y = np.argmax(gamma, axis=1)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n",
    "    # plot the mean of each cluster with striking points\n",
    "    plt.scatter(mu[:, 0], mu[:, 1], c='black', s=50)\n",
    "    plt.axis('equal')\n",
    "    plt.title('iter: {}'.format(iter))\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of EM algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EM(X, k, max_iter=10, plot=True):\n",
    "    \n",
    "    n, d = X.shape\n",
    "    pi = np.ones(k) / k\n",
    "    # k-means++ initialization\n",
    "    mu = np.zeros((k, d))\n",
    "    mu[0] = X[np.random.choice(n)]\n",
    "    for j in range(1, k):\n",
    "        dist = np.zeros(n)\n",
    "        for i in range(n):\n",
    "            dist[i] = np.min(np.sum((X[i] - mu[:j]) ** 2, axis=1))\n",
    "        mu[j] = X[np.random.choice(n, p=dist / np.sum(dist))]\n",
    "    sigma = np.array([np.eye(d) for _ in range(k)])\n",
    "    ll = log_likelihood(X, pi, mu, sigma)\n",
    "\n",
    "    draw(X, pi, mu, sigma, '-1')\n",
    "\n",
    "\n",
    "    for iter in range(max_iter):\n",
    "        # E-step\n",
    "        gamma = np.zeros((n, k))\n",
    "\n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Do the E-step, update p(z_k=1|X). (a.k.a gamma, or the responsibility)       #\n",
    "        # Hint: Refer to the above mentioned algorithm; try use multivariate_normal.   #\n",
    "        ################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "        # M-step\n",
    "\n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Do the M-step, update pi, mu, and sigma.                                     #\n",
    "        # Hint: Refer to the above mentioned algorithm.                                #\n",
    "        ################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        ll_new = log_likelihood(X, pi, mu, sigma)\n",
    "        if np.abs(ll_new - ll) < 1e-5:\n",
    "            break\n",
    "        ll = ll_new\n",
    "\n",
    "        # plot the current cluster\n",
    "        if iter % 1 == 0:\n",
    "            print('Iteration: {}, log-likelihood: {}'.format(iter, ll))\n",
    "            if plot:\n",
    "                draw(X, pi, mu, sigma, iter)\n",
    "\n",
    "    return pi, mu, sigma"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Unsupervised Clustering [40 points]\n",
    "Let's validate our implementation on synthetic data of 2D Gaussian mixture with 5 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is geneated from 5 gaussians with different means, differnet covariance matrices\n",
    "k = 5\n",
    "X = np.loadtxt('synthetic.csv', delimiter=',')\n",
    "plt.scatter(X[:, 0], X[:, 1], s=1)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run EM algorithm\n",
    "pi, mu, sigma = EM(X, k)\n",
    "\n",
    "## A quick validation of your implementation:\n",
    "## log-likelihood should increase after each iteration, and is around ~-20000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following draws the means and clusters of the final result.\n",
    "draw(X, pi, mu, sigma, 'final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following plots the estimated density on a 3D grid.\n",
    "x1 = np.linspace(-3, 12, 100)\n",
    "x2 = np.linspace(-3, 12, 100)\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "Z = np.zeros(X1.shape)\n",
    "for j in range(k):\n",
    "    Z += pi[j] * multivariate_normal(mean=mu[j], cov=sigma[j]).pdf(np.dstack((X1, X2)))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.grid(False)\n",
    "ax.plot_surface(X1, X2, Z)\n",
    "ax.set_xlabel('X1')\n",
    "ax.set_ylabel('X2')\n",
    "ax.set_zlabel('Estimated Density')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Image Compression [20 points]\n",
    "\n",
    "Now let's explore the application of Gaussian mixture model to image compression. We will use the GMM from sklearn package to compress the image as it provides much more stable and faster implementation! The GMM will be trained on the pixels of the image and the cluster assignment for each pixel will be used to replace the original pixel value. The number of clusters is a hyperparameter that can be tuned to control the compression rate. The higher the number of clusters, the higher the compression rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = plt.imread('compression.png')\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image compression using EM in sklearn\n",
    "def GMM_compression(image, k):\n",
    "    X = image.reshape(-1, image.shape[2])\n",
    "    image_compressed = np.zeros(X.shape)\n",
    "    \n",
    "    ##################################################################################################################\n",
    "    # TODO:                                                                                                          #\n",
    "    # Refer to https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html                #\n",
    "    # Create a GaussianMixture object with k components and fit the image data (X).                                  #\n",
    "    # Then, predict the cluster label of each pixel and replace each pixel by its corresponding cluster mean.        #\n",
    "    # Finally, reshape the compressed image to the original image shape.                                             #\n",
    "    ##################################################################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "    original_size = image.shape[0] * image.shape[1] * image.shape[2] * 32\n",
    "    # the compressed image stores the label of each pixel, which can be represented by np.log2(k) bits\n",
    "    # and the mean of each cluster, which can be represented by 4 * 32 bits\n",
    "    compressed_size = image_compressed.shape[0] * image_compressed.shape[1] * np.log2(k) + 4 * 32 * k\n",
    "    compression_rate = original_size / compressed_size\n",
    "    plt.title(f'Compression rate: {compression_rate:.2f}, k={k}')\n",
    "    plt.imshow(image_compressed)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how the image looks like after compression under different number of clusters. Here Compression rate is defined as the theoeretical number of bits required to represent the image divided by the number of bits required to represent the compressed image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [2, 5, 10, 20, 30]:\n",
    "    GMM_compression(image, k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
