%!TEX program = xelatex
\documentclass[10pt]{article}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{titlesec}
\usepackage{xcolor}
\usepackage{enumerate}
\usepackage{bm}
\usepackage{tikz}
\usepackage{listings}
\usetikzlibrary{arrows}
\usepackage{subfigure}
\usepackage{graphicx,booktabs,multirow}
\usepackage[a4paper]{geometry}
\usepackage{upquote}
\usepackage{float}
\usepackage{pdfpages}
\usepackage{amsthm}
\usepackage{bbm}

\geometry{verbose,tmargin=2cm,bmargin=2cm,lmargin=2cm,rmargin=2cm}
\geometry{verbose,tmargin=2cm,bmargin=2cm,lmargin=2cm,rmargin=2cm}
\lstset{language=Matlab}
\lstset{breaklines}

\input defs.tex

\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}

\titleformat*{\section}{\centering\LARGE\scshape}
\renewcommand{\thesection}{\Roman{section}}
\lstset{language=Matlab,tabsize=4,frame=shadowbox,basicstyle=\footnotesize,
keywordstyle=\color{blue!90}\bfseries,breaklines=true,commentstyle=\color[RGB]{50,50,50},stringstyle=\ttfamily,numbers=left,numberstyle=\tiny,
  numberstyle={\color[RGB]{192,92,92}\tiny},backgroundcolor=\color[RGB]{245,245,244},inputpath=code}

\begin{document}

\date{\today}
\title{Introduction to Machine Learning, Fall 2023 \\
	Homework 3\\
	\small (Due Tuesday Nov. 30 at 11:59pm (CST))}
\maketitle

\begin{enumerate}[1.]


	\item \defpoints{15} [Expectation Maximization Algorithm]
            Consider a probabilistic model in which we collectively denote the observed variables by $\boldsymbol{X}$ and all of the hidden variables by $\boldsymbol{Z}$. The joint distribution $p(\boldsymbol{X},\boldsymbol{Z}|\theta)$ is parameterized by $\theta$. Our goal is to maximize the likelihood function given by
            \begin{equation}
                p(\boldsymbol{X}|\theta).
            \end{equation}
 
	      \begin{itemize}
		      \item[(a)] Given an arbitrary distribution $q$, show that the log-likelihood of $\boldsymbol{X}$ is~\defpoints{5}
                    \begin{equation}
                        \log p(\boldsymbol{X}|\theta) = \mathbb{E}_{\boldsymbol{Z}\sim q}\left [ \log  \frac{p(\boldsymbol{X}, \boldsymbol{Z}|\theta)}{q(\boldsymbol{Z})}\right ] + KL\left(q(\boldsymbol{Z})\| p(\boldsymbol{Z}|\boldsymbol{X},\theta)\right).
                    \end{equation}
		      \item[(b)] Next let's consider the expectation step. First show the evidence lower bound (ELBO) is a lower bound of the log-likelihood, namely~\defpoints{5}
                    \begin{equation}
                        \log p(\boldsymbol{X}|\theta)\geq\mathbb{E}_{\boldsymbol{Z}|\boldsymbol{X},\theta^{(t-1)}}\left[\log \frac{p(\boldsymbol{X},\boldsymbol{Z}|\theta)}{p(\boldsymbol{Z}|\boldsymbol{X},\theta^{(t-1)})}\right],
                    \end{equation}
                    where $\theta^{(t-1)}$ is the parameter estimated in the previous iteration.
		      \item[(c)] We want to maximize the ELBO, $\mathbb{E}_{\boldsymbol{Z}|\boldsymbol{X},\theta^{(t-1)}}\left[\log \frac{p(\boldsymbol{X},\boldsymbol{Z}|\theta)}{p(\boldsymbol{Z}|\boldsymbol{X},\theta^{(t-1)})}\right]$ since maximizing $p(\boldsymbol{X}|\theta)$ is hard. EM algorithm defines $Q(\theta|\theta^{(t-1)}) := \mathbb{E}_{\boldsymbol{Z}|\boldsymbol{X},\theta^{(t-1)}}\left[ \log p(\boldsymbol{X},\boldsymbol{Z}|\theta) \right]$. The M-step is given by:
                    \begin{equation}
                        \theta^{(t)} \leftarrow \text{arg}\max_{\theta} Q(\theta|\theta^{(t-1)}). 
                    \end{equation}
                    Show that maximizing $Q(\theta|\theta^{(t-1)})$ and maximizing the ELBO is equivalent.~\defpoints{5} Formally,
                    \begin{equation}
                        \text{arg}\max_{\theta} Q(\theta|\theta^{(t-1)}) = \text{arg}\max_{\theta} \mathbb{E}_{\boldsymbol{Z}|\boldsymbol{X},\theta^{(t-1)}}\left[\log \frac{p(\boldsymbol{X},\boldsymbol{Z}|\theta)}{p(\boldsymbol{Z}|\boldsymbol{X},\theta^{(t-1)})}\right]
                    \end{equation}
			  % \item[(d)] Prove that the likelihood is guaranteed to increase at each iteration, ~\defpoints{3}
     %                \begin{equation}
     %                    p(\boldsymbol{X}|\theta^{(t)}) \geq p(\boldsymbol{X}|\theta^{(t-1)}).
     %                \end{equation}

     
	      \end{itemize}


		  \textbf{Solution:}

(a) With Bayes' Rule, we can get that
$$p(\boldsymbol{X}|\theta)=\dfrac{p(\boldsymbol{X},\boldsymbol{Z}|\theta)}{p(\boldsymbol{Z}|\boldsymbol{X},\theta)}
=\dfrac{p(\boldsymbol{X},\boldsymbol{Z}|\theta)}{q(\boldsymbol{Z})}\dfrac{q(\boldsymbol{Z})}{p(\boldsymbol{Z}|\boldsymbol{X},\theta)}$$

So the log-likelihood of $\boldsymbol{X}$ is
$$\log p(\boldsymbol{X}|\theta)=\log\left [\dfrac{p(\boldsymbol{X},\boldsymbol{Z}|\theta)}{q(\boldsymbol{Z})}\dfrac{q(\boldsymbol{Z})}{p(\boldsymbol{Z}|\boldsymbol{X},\theta)}\right ]
=\log\dfrac{p(\boldsymbol{X},\boldsymbol{Z}|\theta)}{q(\boldsymbol{Z})}+\log\dfrac{q(\boldsymbol{Z})}{p(\boldsymbol{Z}|\boldsymbol{X},\theta)}$$

Take the expectation of $\boldsymbol{Z}$ with respect to $q(\boldsymbol{Z})$ to the both side, we can get that
$$\mathbb{E}_{\boldsymbol{Z}\sim q}\left [\log p(\boldsymbol{X}|\theta)\right ]=\mathbb{E}_{\boldsymbol{Z}\sim q}\left [\log\dfrac{p(\boldsymbol{X},\boldsymbol{Z}|\theta)}{q(\boldsymbol{Z})}+\log\dfrac{q(\boldsymbol{Z})}{p(\boldsymbol{Z}|\boldsymbol{X},\theta)}\right ]$$
With the linearty of expectation:
$$\mathbb{E}_{\boldsymbol{Z}\sim q}\left [\log p(\boldsymbol{X}|\theta)\right ]=\mathbb{E}_{\boldsymbol{Z}\sim q}\left [\log\dfrac{p(\boldsymbol{X},\boldsymbol{Z}|\theta)}{q(\boldsymbol{Z})}\right ]+\mathbb{E}_{\boldsymbol{Z}\sim q}\left [\log\dfrac{q(\boldsymbol{Z})}{p(\boldsymbol{Z}|\boldsymbol{X},\theta)}\right ]$$

For $\mathbb{E}_{\boldsymbol{Z}\sim q}\left [\log p(\boldsymbol{X}|\theta)\right ]$, we can get that it has nothing with $\boldsymbol{Z}$, so
$$\mathbb{E}_{\boldsymbol{Z}\sim q}\left [\log p(\boldsymbol{X}|\theta)\right ]
=\int q(\boldsymbol{z})\log p(\boldsymbol{X}|\theta)d\boldsymbol{z}=\log p(\boldsymbol{X}|\theta)\int q(\boldsymbol{z})d\boldsymbol{z}
=\log p(\boldsymbol{X}|\theta)$$

For $\mathbb{E}_{\boldsymbol{Z}\sim q}\left [\log\dfrac{q(\boldsymbol{Z})}{p(\boldsymbol{Z}|\boldsymbol{X},\theta)}\right ]$,
accoding to the definition of KL divergence : $KL(p\|q)=\int p(\boldsymbol{z})\log\dfrac{p(\boldsymbol{z})}{q(\boldsymbol{z})}d\boldsymbol{z}$, we can get that
$$\mathbb{E}_{\boldsymbol{Z}\sim q}\left [\log\dfrac{q(\boldsymbol{Z})}{p(\boldsymbol{Z}|\boldsymbol{X},\theta)}\right ]
=\int q(\boldsymbol{z})\log\dfrac{q(\boldsymbol{z})}{p(\boldsymbol{z}|\boldsymbol{X},\theta)}d\boldsymbol{z}
=KL(q(\boldsymbol{Z})\|p(\boldsymbol{Z}|\boldsymbol{X},\theta))$$

So above all, we have proved that
$$\log p(\boldsymbol{X}|\theta)=\mathbb{E}_{\boldsymbol{Z}\sim q}\left [\log\dfrac{p(\boldsymbol{X},\boldsymbol{Z}|\theta)}{q(\boldsymbol{Z})}\right ]+KL(q(\boldsymbol{Z})\|p(\boldsymbol{Z}|\boldsymbol{X},\theta))$$

(b) For the log-likelihood:
$$\log p(\boldsymbol{X}|\theta)=\log\int p(\boldsymbol{X},\boldsymbol{z}|\theta)d\boldsymbol{z}$$
$$=\log\int p(\boldsymbol{z}|\boldsymbol{X},\theta^{(t-1)})\dfrac{p(\boldsymbol{X},\boldsymbol{z}|\theta)}{p(\boldsymbol{z}|\boldsymbol{X},\theta^{(t-1)})}d\boldsymbol{z}$$
$$=\log \mathbb{E}_{\boldsymbol{Z}|\boldsymbol{X},\theta^{(t-1)}}\left[\dfrac{p(\boldsymbol{X},\boldsymbol{Z}|\theta)}{p(\boldsymbol{Z}|\boldsymbol{X},\theta^{(t-1)})}\right]$$

Since $\log$ is a concave function, with Jensen's inequality, we have $\log \mathbb{E}(X)\geq \mathbb{E}(\log X)$, so
$$\log \mathbb{E}_{\boldsymbol{Z}|\boldsymbol{X},\theta^{(t-1)}}\left[\dfrac{p(\boldsymbol{X},\boldsymbol{Z}|\theta)}{p(\boldsymbol{Z}|\boldsymbol{X},\theta^{(t-1)})}\right]
\geq \mathbb{E}_{\boldsymbol{Z}|\boldsymbol{X},\theta^{(t-1)}}\left[\log \dfrac{p(\boldsymbol{X},\boldsymbol{Z}|\theta)}{p(\boldsymbol{Z}|\boldsymbol{X},\theta^{(t-1)})}\right]$$

So above all, we have proved that the ELBO is that
$$\log p(\boldsymbol{X}|\theta)\geq\mathbb{E}_{\boldsymbol{Z}|\boldsymbol{X},\theta^{(t-1)}}\left[\log \dfrac{p(\boldsymbol{X},\boldsymbol{Z}|\theta)}{p(\boldsymbol{Z}|\boldsymbol{X},\theta^{(t-1)})}\right]$$

(c)
$$\mathbb{E}_{\boldsymbol{Z}|\boldsymbol{X},\theta^{(t-1)}}\left[\log \dfrac{p(\boldsymbol{X},\boldsymbol{Z}|\theta)}{p(\boldsymbol{Z}|\boldsymbol{X},\theta^{(t-1)})}\right]
=\int p(\boldsymbol{z}|\boldsymbol{X},\theta^{(t-1)})\log \dfrac{p(\boldsymbol{X},\boldsymbol{z}|\theta)}{p(\boldsymbol{z}|\boldsymbol{X},\theta^{(t-1)})}d\boldsymbol{z}$$
$$=\int p(\boldsymbol{z}|\boldsymbol{X},\theta^{(t-1)})(\log p(\boldsymbol{X},\boldsymbol{z}|\theta) - \log  p(\boldsymbol{z}|\boldsymbol{X},\theta^{(t-1)}))d\boldsymbol{z}$$
$$=\int p(\boldsymbol{z}|\boldsymbol{X},\theta^{(t-1)})\log p(\boldsymbol{X},\boldsymbol{z}|\theta)d\boldsymbol{z}-\int p(\boldsymbol{z}|\boldsymbol{X},\theta^{(t-1)})\log p(\boldsymbol{z}|\boldsymbol{X},\theta^{(t-1)})d\boldsymbol{z}$$
Since $Q(\theta|\theta^{(t-1)}):=\mathbb{E}_{\boldsymbol{Z}|\boldsymbol{X},\theta^{(t-1)}}\left[\log p(\boldsymbol{X},\boldsymbol{Z}|\theta)\right]$.\\
So we have
$$\int p(\boldsymbol{z}|\boldsymbol{X},\theta^{(t-1)})\log p(\boldsymbol{X},\boldsymbol{z}|\theta)d\boldsymbol{z}=\mathbb{E}_{\boldsymbol{Z}|\boldsymbol{X},\theta^{(t-1)}}\left[\log p(\boldsymbol{X},\boldsymbol{Z}|\theta)\right]=Q(\theta|\theta^{(t-1)})$$

And with the definition of entropy: $H(\boldsymbol{X})=-\int p(\boldsymbol{x})\log p(\boldsymbol{x})d\boldsymbol{x}$, we can get that
$$-\int p(\boldsymbol{z}|\boldsymbol{X},\theta^{(t-1)})\log p(\boldsymbol{z}|\boldsymbol{X},\theta^{(t-1)})d\boldsymbol{z}=H(\boldsymbol{Z}|\boldsymbol{X},\theta^{(t-1)})$$

So 
$$\int p(\boldsymbol{z}|\boldsymbol{X},\theta^{(t-1)})\log p(\boldsymbol{X},\boldsymbol{z}|\theta)d\boldsymbol{z}-\int p(\boldsymbol{z}|\boldsymbol{X},\theta^{(t-1)})\log p(\boldsymbol{z}|\boldsymbol{X},\theta^{(t-1)})d\boldsymbol{z}$$
$$=Q(\theta|\theta^{(t-1)})+H(\boldsymbol{Z}|\boldsymbol{X},\theta^{(t-1)})$$

Since $H(\boldsymbol{Z}|\boldsymbol{X},\theta^{(t-1)})$ is a constant of $\theta$, so we can get that
$$\argmax\limits_{\theta} \mathbb{E}_{\boldsymbol{Z}|\boldsymbol{X},\theta^{(t-1)}}\left[\log \dfrac{p(\boldsymbol{X},\boldsymbol{Z}|\theta)}{p(\boldsymbol{Z}|\boldsymbol{X},\theta^{(t-1)})}\right]
=\argmax\limits_{\theta} Q(\theta|\theta^{(t-1)}) + H(\boldsymbol{Z}|\boldsymbol{X},\theta^{(t-1)})
=\argmax\limits_{\theta} Q(\theta|\theta^{(t-1)})$$

So above all, we have proved that
$$\argmax\limits_{\theta} Q(\theta|\theta^{(t-1)}) = \argmax\limits_{\theta} \mathbb{E}_{\boldsymbol{Z}|\boldsymbol{X},\theta^{(t-1)}}\left[\log \dfrac{p(\boldsymbol{X},\boldsymbol{Z}|\theta)}{p(\boldsymbol{Z}|\boldsymbol{X},\theta^{(t-1)})}\right]$$
          
	      \newpage

    \item ~\defpoints{15} [Boosting]
        Suppose that we are interested in learning a classifier, such that at any turn of a game we can pose a question, like ``should I attack this ant hill now?", and get an answer.That is, we want to build a classifier which we can feed some features on the current game state, and get the output ``attack" or ``don't attack". There are many possible ways to define what the action ``attack" means, but for now let's define it as sending all friendly ants that can see the ant hill under consideration towards it.

	    Let's recall the AdaBoost algorithm described in class. Its input is a dataset $\{(x_{i},y_{i})\}_{i=1}^{n}$, with $x_i$ being the $i$-th sample, and $y_{i}\in \{-1,1\}$ denoting the $i$-th label, $i=1,2,...,n$. The features might be composed of a count of the number of friendly ants that can see the ant hill under consideration, and a count of the number of enemy ants these friendly ants can see. For example, if there were 10 friendly ants that could see a particular ant hill, and 5 enemy ants that the friendly ants could see, we would have:
	    \begin{align*}
		    x_1 = \begin{bmatrix}
			10 \\
			5
		    \end{bmatrix}.
	    \end{align*}

	    The label of the example $x_{1}$ is $y_{1} = 1$, once the friendly ants were successful in razing the enemy ant hill, and $y_{1} = 0$ otherwise. We could generate such examples by running a greedy bot (or any other opponent bot) against a bot that we periodically try to attack an enemy ant hill. Each time this bot tries the attack, we record (say, after $20$ turns or some other significant amount of time) whether the attack was successful or not.

	    \begin{itemize}
            \item[(a)] Let $\epsilon_t$ denote the error of a weak classifier $h_t$:
		        \begin{equation}
		            \epsilon_{t} = \sum_{i=1}^{n} D_{t}(i) \mathbbm{1}(y_{i} \neq h_{t}(x_{i})).
		        \end{equation}
		        In the simple “attack” / “don't attack” scenario, suppose that we have implemented the following six weak classifiers:
		        \begin{align*}
		            h^{(1)}(x_{i}) = 2 * \mathbbm{1}(x_{i1} \geq 2) - 1, \hspace{1cm}  & h^{(4)}(x_{i}) = 2 * \mathbbm{1}(x_{i2} \leq 2) - 1,  \\
			        h^{(2)}(x_{i}) = 2 * \mathbbm{1}(x_{i1} \geq 6) - 1, \hspace{1cm}  & h^{(5)}(x_{i}) = 2 * \mathbbm{1}(x_{i2} \leq 6) - 1,  \\
			        h^{(3)}(x_{i}) = 2 * \mathbbm{1}(x_{i1} \geq 10) - 1, \hspace{1cm} & h^{(6)}(x_{i}) = 2 * \mathbbm{1}(x_{i2} \leq 10) - 1. \\
		        \end{align*}
		        Given ten training data points ($n = 10$) as shown in Table 1,
		        \begin{table}[t]
                    \caption{The training data in (a).}
                    \label{table1}
                    \centering
                    \begin{tabular}{|c|cc|c|}
                    \hline
                    $i$ & $x_{i1}$ & $x_{i2}$ & $y_{i}$ \\ \hline
                    1 & 1.5 & 0.5 & 1 \\
                    2 & 2.5 & 1.5 & 1 \\
                    3 & 3.5 & 3.5 & 1 \\
                    4 & 6.5 & 5.5 & 1 \\
                    5 & 7.5 & 10.5 & 1 \\
                    6 & 1.5 & 2.5 & -1 \\
                    7 & 3.5 & 1.5 & -1 \\
                    8 & 5.5 & 5.5 & -1 \\
                    9 & 7.5 & 8.5 & -1 \\
                    10 & 1.5 & 10.5 & -1 \\
                    \hline
                    \end{tabular}
                \end{table}
		        please show that what is the minimum value of $\epsilon_{1}$ and which of $h^{(1)},...,h^{(6)}$ achieve this value? Note that there may be multiple classifiers that all have the same $\epsilon_{1}$. You should list all classifiers that achieve the minimum $\epsilon_{1}$ value.~\defpoints{3}\\

	        \item[(b)] For all the questions in the remainder of this section, let $h_{1}$ denote $h^{(1)}$ chosen in the first round of boosting. (That is, $h^{(1)}$ was the classifier that achieved the minimum $\epsilon_{1}$.)
		        \begin{enumerate}
			        \item[(1)] What is the value of $\alpha_{1}$ (the weight of this first classifier $h_{1}$)? ~\defpoints{1}\\

			        \item[(2)] What should $Z_{t}$ be in order to make sure the distribution $D_{t+1}$ is normalized correctly? That is, derive the formula of $Z_{t}$ in terms of $\epsilon_{t}$ that will ensure $\sum_{i=1}^{n} D_{t+1}(i) = 1$. Please also derive the formula of $\alpha_{t}$ in terms of $\epsilon_{t}$. ~\defpoints{3}\\

			        \item[(3)] Which points will increase in significance in the second round of boosting? That is, for which points will we have $D_{1}(i) < D_{2}(i)$? What are the values of $D_{2}$ for these points?~\defpoints{3}\\

			        \item[(4)] In the second round of boosting, the weights on the points will be different, and thus the error $\epsilon_2$ will also be different. Which of $h^{(1)}, . . . , h^{(6)}$ will minimize $\epsilon_2$? (Which classifier will be selected as the second weak classifier $h_2$?) What is its value of $\epsilon_2$?~\defpoints{3}\\

			        \item[(5)] What will the average error of the final classifier $H$ be, if we stop after these two rounds of boosting? That is, if $H(x) = \text{sign}(\alpha_{1}h_{1}(x) + \alpha_{2}h_{2}(x))$, what will the  training error $\epsilon = \frac{1}{n} \sum_{i=1}^{n} \mathbbm{1} (y_{i} \neq H(x_{i}))$ be? Is this more, less, or the same as the error we would get, if we just used one of the weak classifiers instead of this final classifier $H$?~\defpoints{2}\\

		        \end{enumerate}
        \end{itemize}
		\textbf{Solution:}

(a) Since $D_1$ is uniform on the training data, so we have $D_1(i)=\frac{1}{10}$ for $i=1,2,\cdots,10$.\\
So for each classifer $h^{(j)}$, we can get the error $(\epsilon_1)_j$ is
$$(\epsilon_1)_j=\mathbb{E}_{D_1}[\mathbbm{1}(y_{i} \neq h^{(j)}(x_{i}))]=\sum_{i=1}^{n} D_1(i)\mathbbm{1}(y_{i} \neq h^{(j)}(x_{i}))=\dfrac{1}{10}\sum_{i=1}^n\mathbbm{1}(y_{i} \neq h^{(j)}(x_{i}))$$

\begin{itemize}
    \item For $h^{(1)}$, we can get that the data $x_1, x_7, x_8, x_9$ are misclassified, so we have
    $(\epsilon_1)_1=\dfrac{1}{10}\cdot 4=0.4$
    
    \item For $h^{(2)}$, we can get that the data $x_1, x_2, x_3, x_9$ are misclassified, so we have
    $(\epsilon_1)_2=\dfrac{1}{10}\cdot 4=0.4$

    \item For $h^{(3)}$, we can get that the data $x_1, x_2, x_3, x_4, x_5$ are misclassified, so we have
    $(\epsilon_1)_3=\dfrac{1}{10}\cdot 5=0.5$

    \item For $h^{(4)}$, we can get that the data $x_3, x_4, x_5, x_7$ are misclassified, so we have
    $(\epsilon_1)_4=\dfrac{1}{10}\cdot 4=0.4$

    \item For $h^{(5)}$, we can get that the data $x_5, x_6, x_7, x_8$ are misclassified, so we have
    $(\epsilon_1)_5=\dfrac{1}{10}\cdot 4=0.4$

    \item For $h^{(6)}$, we can get that the data $x_5, x_6, x_7, x_8, x_9$ are misclassified, so we have
    $(\epsilon_1)_6=\dfrac{1}{10}\cdot 5=0.5$
\end{itemize}

So above all, the minimum value of $\epsilon_1$ is $0.4$, and the classifiers $h^{(1)}, h^{(2)}, h^{(4)}, h^{(5)}$ achieve this value.\\

(b)\\
(1) From (a), we can get that $\epsilon_1=0.4$.\\
So $\alpha_1=\dfrac{1}{2}\log\dfrac{1-\epsilon_1}{\epsilon_1}=\dfrac{1}{2}\log\dfrac{1-0.4}{0.4}=\dfrac{1}{2}\log\dfrac{3}{2}$.\\

So above all, $\alpha_1=\dfrac{1}{2}\log\dfrac{3}{2}$.\\

(2) 1. To make sure the distribution $D_{t+1}$ is normalized correctly, we should make sure $\sum\limits_{i=1}^{n} D_{t+1}(i) = 1$.\\
Since $D_{t+1}(i)=\dfrac{D_{t}(i)}{Z_t}\exp\left(-\alpha_ty_ih_t(x_i)\right)$, so we have
$$\sum_{i=1}^{n} D_{t+1}(i)=\sum_{i=1}^{n}\dfrac{D_{t}(i)}{Z_t}\exp\left(-\alpha_ty_ih_t(x_i)\right)=\dfrac{1}{Z_t}\sum_{i=1}^{n}D_{t}(i)\exp(-\alpha_ty_ih_t(x_i))=1$$
So we have
\begin{equation}
\begin{aligned}
    Z_t&=\sum_{i=1}^{n}D_{t}(i)\exp\left(-\alpha_ty_ih_t(x_i)\right)\\
    &=\sum_{i:y_i\neq h_t(x_i)}D_{t}(i)e^{\alpha_t}+\sum_{i:y_i=h_t(x_i)}D_{t}(i)e^{-\alpha_t}\\
    &=e^{\alpha_t}\sum_{i=1}^{n}D_{t}(i)\mathbbm{1}(y_{i} \neq h_{t}(x_{i}))+e^{-\alpha_t}\sum_{i=1}^{n}D_{t}(i)\mathbbm{1}(y_{i} = h_{t}(x_{i}))\\
    &=e^{\alpha_t}\epsilon_t+e^{-\alpha_t}(1-\epsilon_t) \hspace{1cm} \text{(From the definition of $\epsilon_t$)}\\
\end{aligned}
\end{equation}






2. Then we need to derive $\alpha_t$ in terms of $\epsilon_t$.\\
Suppose that we have run the AdaBoost algorithm for total $T$ iterations.\\
Let $H_{final}=\text{sign}(\sum\limits_{t=1}^T\alpha_th_t)$\\
So we have the final training error is that\\
\begin{equation}
\begin{aligned}
\epsilon &=\dfrac{1}{n}\sum_{i=1}^n\mathbbm{1}(y_i\neq H_{final}(x_i))\\
& =\frac{1}{n} \sum_{i=1}^n \begin{cases}1 & \text { if } y_i \neq H_{\text {final }}\left(x_i\right) \\
0 & \text { otherwise }\end{cases} \\
& =\frac{1}{n} \sum_{i=1}^n \begin{cases}1 & \text { if } y_i (\sum\limits_{t=1}^T\alpha_th_t) \leq 0 \\
0 & \text { otherwise }\end{cases} \\
& \leq \frac{1}{n} \sum_{i=1}^n \exp \left(-y_i (\sum\limits_{t=1}^T\alpha_th_t)\right) \\
\end{aligned}
\end{equation}

Since we totally have $T$ iterations, so for each iteration, we have
\begin{align*}
    D_{T+1}(i)=\dfrac{D_{T}(i)}{Z_T}\exp(-\alpha_{T}y_ih_{T}(x_i))\\
    D_{T}(i)=\dfrac{D_{T-1}(i)}{Z_{T-1}}\exp(-\alpha_{T-1}y_ih_{T-1}(x_i))\\
    \vdots\\
    D_2(i)=\dfrac{D_1(i)}{Z_2}\exp(-\alpha_1y_ih_1(x_i))\\
    D_1(i)=\dfrac{1}{n}\\
\end{align*} 

Multiply these equations, we can get that
$$D_{T+1}(i)=\dfrac{1}{n}\cdot \prod_{t=1}^{T}\dfrac{1}{Z_t}\exp(-\alpha_ty_ih_t(x_i))=\dfrac{1}{n}\cdot \dfrac{1}{\prod\limits_{t=1}^{T}Z_t}\cdot \exp\left(-y_i\sum_{t=1}^{T}\alpha_th_t(x_i)\right)$$

i.e.
\begin{equation}
\dfrac{1}{n}\cdot \exp\left(-y_i\sum_{t=1}^{T}\alpha_th_t(x_i)\right)=D_{T+1}(i)\prod_{t=1}^{T}Z_t
\end{equation}

If we put the equation (9) into the last of the equation (8), we can get that
$$\epsilon \leq \frac{1}{n} \sum_{i=1}^n \exp \left(-y_i (\sum\limits_{t=1}^{T}\alpha_th_t)\right) =\sum_{i=1}^n D_{T+1}(i)\prod_{t=1}^{T}Z_t=\prod_{t=1}^{T}Z_t\left(\sum_{i=1}^n D_{T+1}(i)\right)$$
Since $Z_t$ is to make sure $D_{t+1}$ is normalized correctly, so we have $\sum\limits_{i=1}^n D_{T+1}(i)=1$, so we have
$$\epsilon \leq \prod_{t=1}^{T}Z_t$$
So if we want to minimize the final error $\epsilon$, we should minimize $\prod\limits_{t=1}^{T}Z_t$.
i.e. we should minimize $Z_t$ for each $t=1,2,\cdots,T$.\\
So for each $Z_t=e^{\alpha_t}\epsilon_t+e^{-\alpha_t}(1-\epsilon_t)$
\begin{align*}
    \dfrac{\partial Z_t}{\partial\alpha_t}&=\epsilon_te^{\alpha_t}-(1-\epsilon_t)e^{-\alpha_t}\\
    \dfrac{\partial^2 Z_t}{\partial\alpha_t^2}&=\epsilon_te^{\alpha_t}+(1-\epsilon_t)e^{-\alpha_t}>0
\end{align*}
So we can get that $Z_t$ is a convex function of $\alpha_t$.\\
So to minimize $Z_t$, we should make $\dfrac{\partial Z_t}{\partial\alpha_t}=0$.\\
i.e. 
\begin{align*}
    \epsilon_te^{\alpha_t}&=(1-\epsilon_t)e^{-\alpha_t}\\
    e^{2\alpha_t}&=\dfrac{1-\epsilon_t}{\epsilon_t}\\
    \alpha_t&=\dfrac{1}{2}\log\dfrac{1-\epsilon_t}{\epsilon_t}
\end{align*}

Since $\epsilon_t=\mathbb{E}_{D_t}[\mathbbm{1}(y_{i} \neq h_{t}(x_{i}))]=P_{D_t}(y_{i} \neq h_{t}(x_{i}))$, so $\epsilon_t\in(0,1)$.\\
So we have $\dfrac{1-\epsilon_t}{\epsilon_t}>0$, so $\log\dfrac{1-\epsilon_t}{\epsilon_t}$ is valid.\\

So we have derived that $\alpha_t=\dfrac{1}{2}\log\dfrac{1-\epsilon_t}{\epsilon_t}$.\\
And put it into the equation (7), we can get that
$$Z_t=\epsilon_te^{\alpha_t}+(1-\epsilon_t)e^{-\alpha_t}=\epsilon_t\sqrt{\dfrac{1-\epsilon_t}{\epsilon_t}}+(1-\epsilon_t)\sqrt{\dfrac{\epsilon_t}{1-\epsilon_t}}=2\sqrt{\epsilon_t(1-\epsilon_t)}$$

So above all, we have derived that
\begin{align*}
    Z_t&=2\sqrt{\epsilon_t(1-\epsilon_t)}\\
    \alpha_t&=\dfrac{1}{2}\log\dfrac{1-\epsilon_t}{\epsilon_t}
\end{align*}

(3) From (2), we can get that $Z_1=2\sqrt{\epsilon_1(1-\epsilon_1)}=2\sqrt{0.4\cdot 0.6}=0.4\sqrt{6}$.\\
And $\alpha_1=\dfrac{1}{2}\log\dfrac{1-\epsilon_1}{\epsilon_1}=\dfrac{1}{2}\log\dfrac{1-0.4}{0.4}=\dfrac{1}{2}\log\dfrac{3}{2}$.\\
Since we take $h_1=h^{(1)}$, so
$$D_2(i)=\dfrac{D_1(i)}{Z_1}\exp\left(-\alpha_1y_ih_1(x_i)\right)=\dfrac{1}{10\cdot 0.4\sqrt{6}}\exp\left(-\dfrac{1}{2}\log\dfrac{3}{2}\cdot y_i\cdot h^{(1)}(x_i)\right)$$
From (a), we can get that for points $x_1,x_7,x_8,x_9$, which are misclassified, so we have $y_i\cdot h^{(1)}(x_i)=-1$.\\
So their weight $D_2(i)=\dfrac{1}{10\cdot 0.4\sqrt{6}}\exp\left(-\dfrac{1}{2}\log\dfrac{3}{2}\cdot (-1)\right)=\dfrac{1}{10\cdot 0.4\sqrt{6}}\cdot \sqrt{\dfrac{3}{2}}=\dfrac{1}{8}>D_1(i)=\dfrac{1}{10}$.\\
And for other points $x_2,x_3,x_4,x_5,x_6,x_{10}$, which are correctly classified, so we have $y_i\cdot h^{(1)}(x_i)=1$.\\
So their weight $D_2(i)=\dfrac{1}{10\cdot 0.4\sqrt{6}}\exp\left(-\dfrac{1}{2}\log\dfrac{3}{2}\cdot 1\right)=\dfrac{1}{10\cdot 0.4\sqrt{6}}\cdot \sqrt{\dfrac{2}{3}}=\dfrac{1}{12}<D_1(i)=\dfrac{1}{10}$.\\

So above all, the misclassified points $x_1,x_7,x_8,x_9$ will increase in significance in the second round of boosting, and their weight $D_2(i)=\dfrac{1}{8}$.\\

(4) From (3), we know that $D_2(1)=D_2(7)=D_2(8)=D_2(9)=\dfrac{1}{8}$,\\
and $D_2(2)=D_2(3)=D_2(4)=D_2(5)=D_2(6)=D_2(10)=\dfrac{1}{12}$.\\
So for each classifer $h^{(j)}$, we can get the error $(\epsilon_2)_j$ is
$$(\epsilon_2)_j=\mathbb{E}_{D_2}[\mathbbm{1}(y_{i} \neq h^{(j)}(x_{i}))]=\sum_{i=1}^{n} D_2(i)\mathbbm{1}(y_{i} \neq h^{(j)}(x_{i}))$$

\begin{itemize}
    \item For $h^{(1)}$, we have $(\epsilon_2)_1=\dfrac{1}{8}\cdot 4+\dfrac{1}{12}\cdot 0=\dfrac{1}{2}$.
    
    \item For $h^{(2)}$, we have $(\epsilon_2)_2=\dfrac{1}{8}\cdot 2+\dfrac{1}{12}\cdot 2=\dfrac{5}{12}$.

    \item For $h^{(3)}$, we have $(\epsilon_2)_3=\dfrac{1}{8}\cdot 1+\dfrac{1}{12}\cdot 4=\dfrac{11}{24}$.

    \item For $h^{(4)}$, we have $(\epsilon_2)_4=\dfrac{1}{8}\cdot 1+\dfrac{1}{12}\cdot 3=\dfrac{3}{8}$.

    \item For $h^{(5)}$, we have $(\epsilon_2)_5=\dfrac{1}{8}\cdot 2+\dfrac{1}{12}\cdot 2=\dfrac{5}{12}$.

    \item For $h^{(6)}$, we have $(\epsilon_2)_6=\dfrac{1}{8}\cdot 3+\dfrac{1}{12}\cdot 2=\dfrac{13}{24}$.
\end{itemize}

So above all, the minimum value of $\epsilon_2$ is $\dfrac{3}{8}$, and the classifier $h^{(4)}$ achieve this value.\\

(5) From (1), we can get that $\alpha_1=\dfrac{1}{2}\log\dfrac{3}{2}$.\\
And from (4), we can get that $\epsilon_2=\dfrac{3}{8}$.\\
So $\alpha_2=\dfrac{1}{2}\log\dfrac{1-\epsilon_2}{\epsilon_2}=\dfrac{1}{2}\log\dfrac{1-\frac{3}{8}}{\frac{3}{8}}=\dfrac{1}{2}\log\dfrac{5}{3}$.\\
And since $h_1(x)=h^{(1)}(x)$ and $h_2(x)=h^{(4)}(x)$, so
$$H(x)=\text{sign}(\alpha_1h_1(x)+\alpha_2h_2(x))=\text{sign}\left(\dfrac{1}{2}\log\dfrac{3}{2}h^{(1)}(x)+\dfrac{1}{2}\log\dfrac{5}{3}h^{(4)}(x)\right)$$
        
There are total $4$ possible combinations of $h^{(1)}(x)$ and $h^{(4)}(x)$, which are $(-1,-1), (1,-1), (-1,1), (1,1)$.\\
So we can get that
\begin{itemize}
    \item For $(-1,-1)$, we have $H(x)=\text{sign}\left(\dfrac{1}{2}\log\dfrac{3}{2}\cdot (-1)+\dfrac{1}{2}\log\dfrac{5}{3}\cdot (-1)\right)=-1$.
    \item For $(1,-1)$, we have $H(x)=\text{sign}\left(\dfrac{1}{2}\log\dfrac{3}{2}\cdot 1+\dfrac{1}{2}\log\dfrac{5}{3}\cdot (-1)\right)=\text{sign}(\dfrac{1}{2}\log\dfrac{9}{10})=-1$.
    \item For $(-1,1)$, we have $H(x)=\text{sign}\left(\dfrac{1}{2}\log\dfrac{3}{2}\cdot (-1)+\dfrac{1}{2}\log\dfrac{5}{3}\cdot 1\right)=\text{sign}(\dfrac{1}{2}\log\dfrac{10}{9})=1$.
    \item For $(1,1)$, we have $H(x)=\text{sign}\left(\dfrac{1}{2}\log\dfrac{3}{2}\cdot 1+\dfrac{1}{2}\log\dfrac{5}{3}\cdot 1\right)=1$.
\end{itemize}

So we can get that $x_3, x_4, x_5, x_7$ are misclassified by $H(x)$(actually, $H(x)$ is exactly same with $h^{(2)}(x)$), so we have
$$\epsilon=\dfrac{1}{n}\sum_{i=1}^n\mathbbm{1}(y_i\neq H(x_i))=\dfrac{1}{10}\cdot 4=0.4$$
And we have $\epsilon_1=\min\limits_{i=1,2,\cdots,6}(\epsilon_1)_i=\min\{0.4, 0.4, 0.5, 0.4, 0.4, 0.5\}=0.4$,
so we can get that $\epsilon=\epsilon_1$.\\

So above all, the average error of the final classifier $H$ is $0.4$, and it is the same as the error we would get, if we just used one of the weak classifiers instead of this final classifier $H$.\\
        
\newpage

	\item \defpoints{10} [Perceptron Learning Algorithm]
            Consider a binary classification problem. The input space is $\mathbb{R}^{d}$. The output space is $\{ +1, -1 \}$. For simplicity, we modified the input to be $\mathbf{x} = [x_0, x_1, \cdots, x_d]^{\intercal}$ with $x_0=1$. The output is predicted using the hypothesis:
            \begin{equation}
                h(\mathbf{x}) = \text{sign}(\mathbf{w}^{\intercal}\mathbf{x}),
            \end{equation}
            where $\mathbf{w} = [w_0, w_1, \cdots, w_d]^{\intercal}$ and $w_0$ is the bias.
            
            The \textit{perceptron learning algorithm} determines $\mathbf{w}$ using a simple iterative method. Here is how it works. At iteration $t$, where $t=0,1,2, \ldots$, there is a current value of the weight vector, call it $\mathbf{w}(t)$. The algorithm picks an example from $\left(\mathbf{x}_1, y_1\right) \cdots\left(\mathbf{x}_N, y_N\right)$ that is currently misclassified, call it $(\mathbf{x}(t), y(t))$, and uses it to update $\mathbf{w}(t)$. Since the example is misclassified, we have $y(t) \neq$ $\operatorname{sign}\left(\mathbf{w}^{\mathrm{T}}(t) \mathbf{x}(t)\right)$. The update rule is

            \begin{equation}
                \mathbf{w}(t+1)=\mathbf{w}(t)+y(t) \mathbf{x}(t).    
            \end{equation}

                
		\begin{itemize}
			\item[(a)] Show that $y(t) \mathbf{w}^{\mathrm{T}}(t) \mathbf{x}(t)<0$. [Hint: $\mathbf{x}(t)$ is misclassified by $\mathbf{w}(t)$.]~\defpoints{3} 
			\item[(b)] Show that $y(t) \mathbf{w}^{\mathrm{T}}(t+1) \mathbf{x}(t)>y(t) \mathbf{w}^{\mathrm{T}}(t) \mathbf{x}(t)$.~\defpoints{3} 
			\item[(c)]   As far as classifying $\mathbf{x}(t)$ is concerned, argue that the move from $\mathbf{w}(t)$ to $\mathbf{w}(t+1)$ is a move ``in the right direction".~\defpoints{4} 
		\end{itemize}

		\textbf{Solution:}
        
(a) Since we are considering the misclassified, so we have $y(t) \neq \operatorname{sign}\left(\mathbf{w}^{\mathrm{T}}(t) \mathbf{x}(t)\right)$.\\
And since $y(t), \operatorname{sign}\left(\mathbf{w}^{\mathrm{T}}(t) \mathbf{x}(t)\right)\in \{+1, -1\}$, so we have $y(t)\cdot\operatorname{sign}\left(\mathbf{w}^{\mathrm{T}}(t)\mathbf{x}(t)\right)=-1<0$.\\
Suppose that $\operatorname{sign}\left(\mathbf{w}^{\mathrm{T}}(t)\mathbf{x}(t)\right)=k\cdot \mathbf{w}^{\mathrm{T}}(t) \mathbf{x}(t)$, where $k>0$.\\
So $y(t)\cdot\operatorname{sign}\left(\mathbf{w}^{\mathrm{T}}(t)\mathbf{x}(t)\right)=y(t)\cdot k\cdot \mathbf{w}^{\mathrm{T}}(t) \mathbf{x}(t)=k\cdot y(t) \mathbf{w}^{\mathrm{T}}(t) \mathbf{x}(t)<0$.\\
Since $k>0$, so we have
$$y(t) \mathbf{w}^{\mathrm{T}}(t) \mathbf{x}(t)<0$$
So above all, we have proved that $y(t) \mathbf{w}^{\mathrm{T}}(t) \mathbf{x}(t)<0$.

(b) Since we are considering the misclassified, so we have $\mathbf{w}(t+1)=\mathbf{w}(t)+y(t) \mathbf{x}(t)$.\\
So
$$y(t)\mathbf{w}^T(t+1)\mathbf{x}(t)=y(t)\mathbf{w}^T(t)\mathbf{x}(t)+y(t)y(t)\mathbf{x}^T(t)\mathbf{x}(t)
=y(t)\mathbf{w}^T(t)\mathbf{x}(t)+y^2(t)\|\mathbf{x}(t)\|^2$$
Since $y(t)\in \{+1, -1\}$, so we have $y^2(t)=1$.\\
And since for the simplicity, we have the input to be $\mathbf{x} = [x_0, x_1, \cdots, x_d]^{\intercal}$ with $x_0=1$, so we have
$\mathbf{x}\neq\mathbf{0}$, i.e. $\|\mathbf{x}(t)\|^2>0$.\\
So we have 
$$y^2(t)\|\mathbf{x}(t)\|^2>0$$
So
$$y(t)\mathbf{w}^T(t+1)\mathbf{x}(t)=y(t)\mathbf{w}^T(t)\mathbf{x}(t)+y^2(t)\|\mathbf{x}(t)\|^2>y(t)\mathbf{w}^T(t)\mathbf{x}(t)$$
So above all, we have proved that $y(t) \mathbf{w}^{\mathrm{T}}(t+1) \mathbf{x}(t)>y(t) \mathbf{w}^{\mathrm{T}}(t) \mathbf{x}(t)$.

(c) We only consider about the misclassified case.\\
From (a), we knew that $$y(t) \mathbf{w}^{\mathrm{T}}(t) \mathbf{x}(t)<0$$
And from (b), we knew that $$y(t) \mathbf{w}^{\mathrm{T}}(t+1) \mathbf{x}(t)>y(t) \mathbf{w}^{\mathrm{T}}(t) \mathbf{x}(t)$$
So we could see that the move from $\mathbf{w}(t)$ to $\mathbf{w}(t+1)$ is making the $y(t)\mathbf{w}^T\mathbf{x}(t)$ to the more positive direction, 
and since if $y(t)\mathbf{w}^T\mathbf{x}(t)>0$, then it is a correct classification.\\ 
And if the total input data are linearly separable, from what we have learned, we could get that with 
at most $M=(\frac{R}{\gamma})^2$ such misclassified's movement, where $R$ is the radius of the smallest sphere that contains all the input data, and $\gamma$ is the margin,
then we could get the correct classification.\\

So above all, we could say that the move from $\mathbf{w}(t)$ to $\mathbf{w}(t+1)$ is a move ``in the right direction".

\end{enumerate}

\end{document}