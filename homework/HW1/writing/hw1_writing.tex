%!TEX program = xelatex
\documentclass[10pt]{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{titlesec}
\usepackage{xcolor}
\usepackage{enumerate}
\usepackage{bm}
\usepackage{tikz}
\usepackage{listings}
\usetikzlibrary{arrows}
\usepackage{subfigure}
\usepackage{graphicx,booktabs,multirow}
\usepackage[a4paper]{geometry}
\usepackage{upquote}
\usepackage{float}
\usepackage{pdfpages}

\geometry{verbose,tmargin=2cm,bmargin=2cm,lmargin=2cm,rmargin=2cm}
\geometry{verbose,tmargin=2cm,bmargin=2cm,lmargin=2cm,rmargin=2cm}
\lstset{language=Matlab}
\lstset{breaklines}

\input defs.tex

\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}

\titleformat*{\section}{\centering\LARGE\scshape}
\renewcommand{\thesection}{\Roman{section}}
\lstset{language=Matlab,tabsize=4,frame=shadowbox,basicstyle=\footnotesize,
keywordstyle=\color{blue!90}\bfseries,breaklines=true,commentstyle=\color[RGB]{50,50,50},stringstyle=\ttfamily,numbers=left,numberstyle=\tiny,
  numberstyle={\color[RGB]{192,92,92}\tiny},backgroundcolor=\color[RGB]{245,245,244},inputpath=code}

\begin{document}

\date{\today}
\title{Introduction to Machine Learning, Fall 2023 \\
	Homework 1\\
	\small (Due Thursday, Oct. 26 at 11:59pm (CST))}
\maketitle
\begin{enumerate}[1.]


	\item \defpoints{10} [Math review] Suppose $\{\mathbf{X}_1, \mathbf{X}_2, \cdots, \mathbf{X}_n\}$ are random samples from a random variable $\mathbf{X}$:
	      \begin{itemize}
		      \item[(a)] Prove that the covariance of $\mathbf{X}$ is a semi positive definite matrix. ~\defpoints{3}
		      \item[(b)] Assuming $\mathbf{X}\sim \mathcal{N}(\mathbf{\mu},\mathbf{\Sigma})$ which is a multivariate normal distribution, derive the the log-likelihood $\mathit{l}(\mathbf{\mu},\mathbf{\Sigma})$ and MLE of $\mathbf{\mu}$ ~\defpoints{4}
		      \item[(c)] Suppose $\hat{\theta}$ is an unbiased estimator of $\theta$ and $\mathbf{Var}(\hat{\theta})>0$. Prove that $(\hat{\theta})^2$ is not an unbiased estimator of $\theta^2$. ~\defpoints{3}
	      \end{itemize}

(a) $\mathbf{\Sigma}=E[(\mathbf{X}-\mu)(\mathbf{X}-\mu)^T]$\\
Suppose that the dimension of $\mathbf{X}$ is $p$.\\
So $\forall \mathbf{y}\in\mathbb{R}^p$,\\
$\mathbf{y}^T\mathbf{\Sigma}\mathbf{y}=\mathbf{y}^TE[(\mathbf{X}-\mu)(\mathbf{X}-\mu)^T]\mathbf{y}=E[\mathbf{y}^T(\mathbf{X}-\mu)(\mathbf{X}-\mu)^Ty]$\\
$=E[((\mathbf{X}-\mu)^T\mathbf{y})^T((\mathbf{X}-\mu)^T\mathbf{y})]=E(\|(\mathbf{X}-\mu)^T\mathbf{y}\|_2^2)$.\\
since $\|(\mathbf{X}-\mu)^T\mathbf{y}\|_2^2\geq0$, so $E(\|(\mathbf{X}-\mu)^T\mathbf{y}\|_2^2)\geq0$\\
so $\forall \mathbf{y}\in\mathbb{R}^p,\mathbf{y}^T\mathbf{\Sigma} \mathbf{y}\geq 0$\\
So $\mathbf{\Sigma}$ is a semi positive definite matrix.\\
So above all, the covariance of $\mathbf{X}, \mathbf{\Sigma}$ is a semi positive definite matrix.\\

(b) From what we have learned in class, the PDF of the multivariate normal distribution
$\mathbf{X}_i\sim \mathcal{N}(\mu, \mathbf{\Sigma})$ is that $Pr(\mathbf{X}_i;\mu,\mathbf{\Sigma})=\frac{1}{(2\pi)^\frac{p}{2}|\mathbf{\Sigma}|^{\frac{1}{2}}}exp(-\dfrac{1}{2}(\mathbf{X}_i-\mu)^T\mathbf{\Sigma}^{-1}(\mathbf{X}_i-\mu))$
, suppose that the dimension of $\mathbf{X}_i$ is $p$.\\
Since the sampling are independent, so the likelihood function is:\\
$Pr(\mathbf{X}_1,\cdots,\mathbf{X}_n;\mu,\mathbf{\Sigma})=\prod\limits_{i=1}^nPr(\mathbf{X}_i;\mu,\mathbf{\Sigma})$\\
Then the log-likelihood function is:\\
$l(\mu,\mathbf{\Sigma})=\sum\limits_{i=1}^n\log(\frac{1}{(2\pi)^\frac{p}{2}|\mathbf{\Sigma}|^{\frac{1}{2}}}exp(-\dfrac{1}{2}(\mathbf{X}_i-\mu)^T\mathbf{\Sigma}^{-1}(\mathbf{X}_i-\mu)))$\\
$=n\cdot\log(\frac{1}{(2\pi)^\frac{p}{2}|\mathbf{\Sigma}|^{\frac{1}{2}}})-\sum\limits_{i=1}^n\dfrac{1}{2}(\mathbf{X}_i-\mu)^T\mathbf{\Sigma}^{-1}(\mathbf{X}_i-\mu)$\\

And the MLE of $\mu$ is:\\
$\hat{\mu}=\argmax\limits_{\mu}Pr(\mathbf{X}_1,\cdots,\mathbf{X}_n;\mu,\mathbf{\Sigma})=\argmax\limits_{\mu}l(\mu,\mathbf{\Sigma})$\\
Since $l(\mu,\mathbf{\Sigma})$ is a concave function, so we can get the optimal solution by setting the derivative of $l(\mu,\mathbf{\Sigma})$ to 0.\\
$\frac{\partial l(\mu,\mathbf{\Sigma})}{\partial \mu}=\frac{\partial n\cdot\log(\frac{1}{(2\pi)^\frac{p}{2}|\mathbf{\Sigma}|^{\frac{1}{2}}})}{\partial \mu}-\frac{\partial \sum\limits_{i=1}^n\dfrac{1}{2}(\mathbf{X}_i-\mu)^T\mathbf{\Sigma}^{-1}(\mathbf{X}_i-\mu)}{\partial \mu}$\\
$=\sum\limits_{i=1}^n-\frac{\partial \dfrac{1}{2}(\mathbf{X}_i-\mu)^T\mathbf{\Sigma}^{-1}(\mathbf{X}_i-\mu)}{\partial \mu}$\\

Since $\mathbf{\Sigma}$ is the covariance matrix, so $\mathbf{\Sigma}$ is a symmetric matrix, i.e.$\mathbf{\Sigma}=\mathbf{\Sigma}^T$.\\
So $(\mathbf{\Sigma}^{-1})^T=(\mathbf{\Sigma}^T)^{-1}=\mathbf{\Sigma}^{-1}$.\\
i.e. $\mathbf{\Sigma}$ is also a symmetric matrix.\\

So $\frac{\partial l(\mu,\mathbf{\Sigma})}{\partial \mu}=\sum\limits_{i=1}^n-\frac{\partial \dfrac{1}{2}(\mathbf{X}_i-\mu)^T\mathbf{\Sigma}^{-1}(\mathbf{X}_i-\mu)}{\partial \mu}$\\
$=\sum\limits_{i=1}^n-\frac{\partial \dfrac{1}{2}(\mathbf{X}_i-\mu)^T\mathbf{\Sigma}^{-1}(\mathbf{X}_i-\mu)}{\partial (\mathbf{X}_i-\mu)}\frac{\partial (\mathbf{X}_i-\mu)}{\partial \mu}$\\
$=\sum\limits_{i=1}^n-\dfrac{1}{2}(2\mathbf{\Sigma}^{-1}(\mathbf{X}-\mu))(-1)$\\
$=\sum\limits_{i=1}^n\mathbf{\Sigma}^{-1}(\mathbf{X}_i-\mu)$\\
$=\mathbf{\Sigma}^{-1}(\sum\limits_{i=1}^n\mathbf{X}_i-n\mu)$\\

So $\frac{\partial l(\mu,\mathbf{\Sigma})}{\partial \mu}=0\Rightarrow \mathbf{\Sigma}^{-1}(\sum\limits_{i=1}^n\mathbf{X}_i-n\mu)=0\Rightarrow\hat{\mu}=\dfrac{1}{n}\sum\limits_{i=1}^n\mathbf{X}_i$\\

So above all, the log-likelihood function is $l(\mu,\mathbf{\Sigma})=n\cdot\log(\frac{1}{(2\pi)^\frac{p}{2}|\mathbf{\Sigma}|^{\frac{1}{2}}})-\sum\limits_{i=1}^n\dfrac{1}{2}(\mathbf{X}_i-\mu)^T\mathbf{\Sigma}^{-1}(\mathbf{X}_i-\mu)$\\
And the MLE of $\mu$ is $\hat{\mu}=\dfrac{1}{n}\sum\limits_{i=1}^n\mathbf{X}_i$\\

(c) Since $\hat{\theta}$ is the unbiased estimator of $\theta$, so $E(\hat{\theta})=\theta$.\\
And from the defination of varaiance, we could get that $Var(\hat{\theta})=E[(\hat{\theta}-E(\hat{\theta}))^2]=E[(\hat{\theta})^2]-[E(\hat{\theta})]^2$.\\
Since $Var(\hat{\theta})>0$, so $E[(\hat{\theta})^2]-[E(\hat{\theta})]^2>0$\\
i.e. $E[(\hat{\theta})^2]>[E(\hat{\theta})]^2=(\hat{\theta})^2$\\
So $E[(\hat{\theta})^2]\neq(\hat{\theta})^2$\\
So $(\hat{\theta})^2$ is not an unbiased estimator of $\theta^2$.\\

So above all, we have proved that if $\hat{\theta}$ is an unbiased estimator of $\theta$ and $Var(\hat{\theta})>0$, then $(\hat{\theta})^2$ is not an unbiased estimator of $\theta^2$.\\



	      \newpage

	\item \defpoints{10} Consider real-valued variables $X$ and $Y$, in which $Y$ is generated conditional on $X$ according to
	$$
	Y = aX + b + \epsilon, \ \text{where} \ \epsilon \sim \mathcal{N}(0, \sigma^2).
	$$
	Here $\epsilon$ is an independent variable, called a noise term, which is drawn from a Gaussian distribution with mean 0,
	and variance $\sigma^2$. This is a single variable linear regression model, where $a$ is the only weight parameter and $b$ denotes the intercept.
	The conditional probability of $Y$ has a distribution $p(Y | X, a, b) \sim \mathcal{N}(aX+b, \sigma^2)$, so it can be written as:
	$$
	p(Y|X, a,b) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{1}{2\sigma^2}(Y - aX -b)^2\right).
	$$
	\begin{itemize}
		\item[(a)] Assume we have a training dataset of $n$ i.i.d. pairs $(x_i, y_i)$, $i = 1, 2, ..., n$, and
		the likelihood function is defined by $L(a,b) = \prod_{i=1}^n p(y_i | x_i, a, b)$. Please write the
		Maximum Likelihood Estimation (MLE) problem for estimating $a$ and $b$.~\defpoints{3}
		\item[(b)] Estimate the optimal solution of $a$ and $b$ by solving the MLE problem in (a).~\defpoints{4}
		\item[(c)] Based on the result in (b), argue that the learned linear model $f(X) = aX + b$,
		always passes through the point $(\bar{x},\bar{y})$,
		where $\bar{x} = \tfrac{1}{n}\sum_{i=1}^{n}x_{i}$ and $\bar{y} = \tfrac{1}{n}\sum_{i=1}^{n}y_{i}$ denote the sample means.~\defpoints{3}
	\end{itemize}


(a) the MLE of $a$ and $b$ is:\\
$\hat{a},\hat{b}=\argmax\limits_{a,b}L(a,b)=\argmax\limits_{a,b}\prod\limits_{i=1}^np(y_i|x_i,a,b)$
$=\argmax\limits_{a,b}\prod\limits_{i=1}^n\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{1}{2\sigma^2}(y_i-ax_i-b)^2)$\\

So above all, the MLE problem for estimating $a$ and $b$ is:\\
$\hat{a},\hat{b}=\argmax\limits_{a,b}\prod\limits_{i=1}^n\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{1}{2\sigma^2}(y_i-ax_i-b)^2)$\\

(b) 
Take log to the likelihood function, we could get:\\
$\hat{a},\hat{b}=\argmax\limits_{a,b}\sum\limits_{i=1}^n\log(p(y_i|x_i,a,b))=\argmax\limits_{a,b}\sum\limits_{i=1}^n\log(\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{1}{2\sigma^2}(y_i-ax_i-b)^2))$\\
Since $\frac{1}{\sqrt{2\pi}\sigma}$ has nothing with $a,b$, and $\sigma$ is just the variance of the noise term, so $\frac{1}{\sqrt{2\pi}\sigma}, -\dfrac{1}{2\sigma^2}$ are just constants.\\
so $\hat{a},\hat{b}=\argmax\limits_{a,b}\sum\limits_{i=1}^n-(y_i-ax_i-b)^2=\argmin\limits_{a,b}\sum\limits_{i=1}^n(y_i-ax_i-b)^2$\\
Since $\sum\limits_{i=1}^n(y_i-ax_i-b)^2$ is a convex function both for $a$ and $b$, so we just need to set the derivative of $\sum\limits_{i=1}^n(y_i-ax_i-b)^2$ to 0 to get the optimal solution.\\
Let $f(a,b)=\sum\limits_{i=1}^n(y_i-ax_i-b)^2,\bar{x}=\dfrac{1}{n}\sum\limits_{i=1}^nx_i,\bar{y}=\dfrac{1}{n}\sum\limits_{i=1}^ny_i$\\

So $\frac{\partial f}{\partial b}=\sum\limits_{i=1}^n-2(y_i-ax_i-b)=2nb-2\sum\limits_{i=1}^n(y_i-ax_i)$\\
$\dfrac{\partial f}{\partial b}=0 \Rightarrow 2nb=2\sum\limits_{i=1}^n(y_i-ax_i)\Rightarrow b=\dfrac{1}{n}\sum\limits_{i=1}^ny_i-\dfrac{1}{n}a\sum\limits_{i=1}^nx_i$\\
$\Rightarrow b=\bar{y}-a\bar{x}$\\

Similarly, $\frac{\partial f}{\partial a}=\sum\limits_{i=1}^n-2x_i(y_i-ax_i-b)=(-2)\sum\limits_{i=1}^nx_iy_i-(-2)\sum\limits_{i=1}^nax_i^2-(-2)\sum\limits_{i=1}^nbx_i$\\
$\dfrac{\partial f}{\partial a}=0\Rightarrow \sum\limits_{i=1}^nx_iy_i-a\sum\limits_{i=1}^nx_i^2-b\sum\limits_{i=1}^nx_i=0$\\
put $b=\bar{y}-a\bar{x}$ into the above equation, we could get:\\
$\Rightarrow \sum\limits_{i=1}^nx_iy_i-a\sum\limits_{i=1}^nx_i^2-(\bar{y}-a\bar{x})\sum\limits_{i=1}^nx_i=0$\\
$\Rightarrow a=\dfrac{\sum\limits_{i=1}^nx_iy_i-n\bar{x}\bar{y}}{\sum\limits_{i=1}^nx_i^2-n(\bar{x})^2}$\\

And put $a=\dfrac{\sum\limits_{i=1}^nx_iy_i-n\bar{x}\bar{y}}{\sum\limits_{i=1}^nx_i^2-n(\bar{x})^2}$ into $b=\bar{y}-a\bar{x}$, we could get:\\
$b=\bar{y}-\dfrac{\sum\limits_{i=1}^nx_iy_i-n\bar{x}\bar{y}}{\sum\limits_{i=1}^nx_i^2-n(\bar{x})^2}\bar{x}=\dfrac{\sum\limits_{i=1}^nx_i^2\bar{y}-\sum\limits_{i=1}^nx_iy_i\bar{x}}{\sum\limits_{i=1}^nx_i^2-n(\bar{x})^2}$\\

So above all, the optimal solution of $a$ and $b$ is:\\
$a=\dfrac{\sum\limits_{i=1}^nx_iy_i-n\bar{x}\bar{y}}{\sum\limits_{i=1}^nx_i^2-n(\bar{x})^2}$\\
$b=\bar{y}-a\bar{x}=\dfrac{\sum\limits_{i=1}^nx_i^2\bar{y}-\sum\limits_{i=1}^nx_iy_i\bar{x}}{\sum\limits_{i=1}^nx_i^2-n(\bar{x})^2}$\\

(c) From the analysis in (b), we could get that:\\
$\dfrac{\partial f}{\partial b}=2nb-2\sum\limits_{i=1}^n(y_i-ax_i)=0\Rightarrow b=\bar{y}-a\bar{x}$\\
i.e. $b=\bar{y}-a\bar{x}$.\\

Put $(\bar{x},\bar{y})$ into the linear model $f(X)=aX+b$, we could get:\\
$f(\bar{x})=a\bar{x}+b=a\bar{x}+\bar{y}-a\bar{x}=\bar{y}$\\

So above all, the learned linear model $f(X)=aX+b$ always passes through the point $(\bar{x},\bar{y})$.\\

	      \newpage

	\item \defpoints{10} [Regression and Classification]
	      \begin{itemize}
			\item[(a)] When we talk about linear regression, what does `linear' regard to? \defpoints{2}
			\item[(b)] Assume that there are $n$ given training examples $\{(x_1, y_1), (x_2, y_2), \cdots, (x_n, y_n)\}$,
			where each input data point $x_i$ has $m$ real valued features. When $m > n$, the linear regression model
			is equivalent to solving an under-determined system of linear equations $\mathbf{y} = \mathbf{X}\beta$. One popular way to
			estimate $\beta$ is to consider the so-called ridge regression:
			\[\argmin_{\beta} ||\mathbf{y}-\mathbf{X}\mathbf{\beta}||_2^2 + \lambda||\beta||_2^2\]
			for some $\lambda > 0$. This is also known as Tikhonov regularization.
			
			Show that the optimal solution $\beta_*$ to the above optimization problem is given by
			\[\mathbf{\beta}_* = (\mathbf{X}^T\mathbf{X} + \lambda \mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}\]
			Hint: You need to prove that given $\lambda>0$, $\mathbf{X}^T\mathbf{X} + \lambda \mathbf{I}$ is invertible. \defpoints{5}
			\item[(c)] Is the given data set linear separable? If yes, construct a linear hypothesis function to separate the given data set. If no, explain the reason. \defpoints{3}
			
			\begin{table}[h]
				\centering
				\begin{tabular}{c|cccccc}
					Data & (1,3) & (4,4) & (3,-6) & (-2,1) & (-3,5) & (-6,-4) \\ \hline
					Label & +1 & -1 & -1 & +1 & -1 & -1
				\end{tabular}
				\label{tab:my_label}
			\end{table}
	      \end{itemize}
      	  
(a) Linear is to the for all parameters of the regression variable $\beta$.\\

(b) As we have learned in linear algebra, we know that tha matrix
$X^TX$ must be similiar and diagonalizable.\\
i.e. there must exist a matrix $P$ and a diagonal matrix $\Lambda$ such that $X^TX=P\Lambda P^{-1}$.\\
Also $\forall x\in\mathbb{R}^n$, we have $x^T(\mathbf{X}^T\mathbf{X}) x=(\mathbf{X}x)^T(\mathbf{X}x)=\|\mathbf{X}x\|_2^2\geq0$.\\
So $X^TX$ is positive semi-definite.\\
So all eigenvalues of $X^TX$ are non-negative.\\
i.e. the diagonal matrix $\Lambda$'s elements are all positive.

And since $\lambda>0$, so $\lambda I$'s all elements are all also non-negative, and $\lambda I$ is also a diagonal matrix.\\
So $X^TX+\lambda I=P\Lambda P^{-1}+\lambda PIP^{-1}=P(\Lambda+\lambda I)P^{-1}$.\\
Since $\Lambda, \lambda I$ are all diagonal matrix, so $\Lambda+\lambda I$ is also a diagonal matrix.\\
And all elements in $\Lambda+\lambda I$ are all positive, this is because in $Lambda$, elements are non-nagetiva, in $\lambda I$, all elements are positive.
So $\Lambda+\lambda I$ is positive defined.\\
Since $X^TX+\lambda I=P(\Lambda+\lambda I)P^{-1}$,
from the knowledge of similarity and diagonalizable, we could know that $X^TX+\lambda I$ is also positive defined.\\
So $X^TX+\lambda I$ is invertible.\\
And let $f(\beta)=\|\mathbf{y}-\mathbf{X}\mathbf{\beta}\|_2^2 + \lambda\|\beta\|_2^2=(\mathbf{y}-\mathbf{X}\mathbf{\beta})^T(\mathbf{y}-\mathbf{X}\mathbf{\beta})+\lambda\beta^T\beta=\mathbf{y}^T\mathbf{y}-\beta^T\mathbf{X}^T\mathbf{y}-\mathbf{y}^T\mathbf{X}\beta+\beta^T\mathbf{X}^T\mathbf{X}\beta+\lambda\beta^T\beta$\\
Since $f(\beta)$ is convex, so we just need to set the derivative of $f(\beta)$ to 0 to get the optimal solution.\\
$\frac{\partial f(\beta)}{\partial \beta}=2(-\mathbf{X}^T\mathbf{y}+\mathbf{X}^T\mathbf{X}\beta+\lambda\beta)$\\
$\frac{\partial f(\beta)}{\partial \beta}=0\Rightarrow (\mathbf{X}^\mathbf{X}+\lambda I)\beta=\mathbf{X}^T\mathbf{y}\Rightarrow \beta=(\mathbf{X}^T\mathbf{X}+\lambda I)^{-1}\mathbf{X}^T\mathbf{y}$\\

Since we have proved that $X^TX+\lambda I$ is invertible, so $(\mathbf{X}^T\mathbf{X}+\lambda I)^{-1}$ exists.
So $\beta *=(\mathbf{X}^T\mathbf{X}+\lambda I)^{-1}\mathbf{X}^T\mathbf{y}$\\

So above all, the optimal solution $\beta *=(\mathbf{X}^T\mathbf{X}+\lambda I)^{-1}\mathbf{X}^T\mathbf{y}$.\\

(c) Let the data be formed as $\mathbf{X}_i=(x_1,x_2)$.\\
And let the hypothesis function be $f(\mathbf{X})=b_1x_1^2+b_2x_2^2+b_3$.\\
Let $b_1=1,b_2=1,b_3=-25$, so the regression function is $f(\mathbf{X})=x_1^2+x_2^2-25$, and its a linear regression.\\
Make the separate line be $f(\mathbf{X})=0$, so the separate line is $x_1^2+x_2^2-25=0$.\\
If $f(\mathbf{X})\leq 0$, set the label to be $+1$, else set the label to be $-1$.\\
And we could get the result as below:\\

	\begin{table}[H]
		\centering
		\begin{tabular}{c|cccccc}
			Data $\mathbf{X}$ & (1,3) & (4,4) & (3,-6) & (-2,1) & (-3,5) & (-6,-4) \\ \hline
			Function value $f(\mathbf{X})$ & -15 & 7 & 20 & -20 & 9 & 27\\
			Label & +1 & -1 & -1 & +1 & -1 & -1
		\end{tabular}
		\label{tab:my_label}
	\end{table}

So above all, we could find that we can construct a linear hypothesis function to separate the given data set.\\
	
	\newpage


\end{enumerate}

\end{document}