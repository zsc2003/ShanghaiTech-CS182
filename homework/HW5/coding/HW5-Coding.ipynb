{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z_LTBtfMBvW_"
   },
   "source": [
    "# Homework 5: Convolutional neural network (30 points)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you need to implement and train a convolutional neural network on the CIFAR-10 dataset with PyTorch.\n",
    "### What is PyTorch?\n",
    "\n",
    "PyTorch is a system for executing dynamic computational graphs over Tensor objects that behave similarly as numpy ndarray. It comes with a powerful automatic differentiation engine that removes the need for manual back-propagation. \n",
    "\n",
    "### Why?\n",
    "\n",
    "* Our code will now run on GPUs! Much faster training. When using a framework like PyTorch or TensorFlow you can harness the power of the GPU for your own custom neural network architectures without having to write CUDA code directly (which is beyond the scope of this class).\n",
    "* We want you to be ready to use one of these frameworks for your project so you can experiment more efficiently than if you were writing every feature you want to use by hand. \n",
    "* We want you to stand on the shoulders of giants! TensorFlow and PyTorch are both excellent frameworks that will make your lives a lot easier, and now that you understand their guts, you are free to use them :) \n",
    "* We want you to be exposed to the sort of deep learning code you might run into in academia or industry.\n",
    "## How can I learn PyTorch?\n",
    "\n",
    "Justin Johnson has made an excellent [tutorial](https://github.com/jcjohnson/pytorch-examples) for PyTorch. \n",
    "\n",
    "You can also find the detailed [API doc](http://pytorch.org/docs/stable/index.html) here. If you have other questions that are not addressed by the API docs, the [PyTorch forum](https://discuss.pytorch.org/) is a much better place to ask than StackOverflow."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T33dD1e8tii2"
   },
   "source": [
    "Install PyTorch and Skorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "pJB3VQYDCUmh",
    "outputId": "43ab27ff-b2fb-4f7b-f517-f2b029b18ba2",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q torch skorch torchvision torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "3l_Dl6qxCXmv",
    "outputId": "c0bbb1f4-081a-464e-bf46-a34c2767e83a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import skorch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uevQtU7NtZ_-"
   },
   "source": [
    "## 0. Tensor Operations (5 points)\n",
    "\n",
    "Tensor operations are important in deep learning models. In this part, you are required to get famaliar to some common tensor operations in PyTorch."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5DeQOItkeQCx"
   },
   "source": [
    "### 1) Tensor squeezing, unsqueezing and viewing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pAOmBE5ODwpP"
   },
   "source": [
    "Tensor squeezing, unsqueezing and viewing are important methods to change the dimension of a Tensor, and the corresponding functions are [torch.squeeze](https://pytorch.org/docs/stable/torch.html#torch.squeeze), [torch.unsqueeze](https://pytorch.org/docs/stable/torch.html#torch.unsqueeze) and [torch.Tensor.view](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view). Please read the documents of the functions, and finish the following practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "hVrM80YxFSjb",
    "outputId": "933f2576-b413-4516-a3f8-5fa4fbe60b38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 2, 1])\n",
      "torch.Size([3, 2])\n",
      "torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "# x is a tensor with size being (3, 2)\n",
    "x = torch.Tensor([[1, 2], \n",
    "                  [3, 4], \n",
    "                  [5, 6]])\n",
    "x.shape\n",
    "\n",
    "# Add two new dimensions to x by using the function torch.unsqueeze, so that the size of x becomes (3, 1, 2, 1).\n",
    "x = torch.unsqueeze(x, 1)\n",
    "x = torch.unsqueeze(x, 3)\n",
    "print(x.shape)\n",
    "\n",
    "# Remove the two dimensions justed added by using the function torch.squeeze, and change the size of x back to (3, 2).\n",
    "x = torch.squeeze(x, 3)\n",
    "x = torch.squeeze(x, 1)\n",
    "# x = torch.squeeze(x, (0, 2)) # in torch 2.0 this will work \n",
    "print(x.shape)\n",
    "\n",
    "# x is now a two-dimensional tensor, or in other words a matrix. Now use the function torch.Tensor.view and change x to a one-dimensional vector with size being (6).\n",
    "x = x.view(6)\n",
    "print(x.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "liuR-U0wea0n"
   },
   "source": [
    "### 2) Tensor concatenation and stack"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pkbnt6v8Bo-j"
   },
   "source": [
    "Tensor concatenation and stack are operations to combine small tensors into big tensors. The corresponding functions are [torch.cat](https://pytorch.org/docs/stable/torch.html#torch.cat) and [torch.stack](https://pytorch.org/docs/stable/torch.html#torch.stack). Please read the documents of the functions, and finish the following practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "b9KqXu3Stfjh",
    "outputId": "97f17754-1e49-4a77-df8a-46a1200ea170"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[-1., -2.],\n",
      "        [-3., -4.],\n",
      "        [-5., -6.]])\n"
     ]
    }
   ],
   "source": [
    "# x is a tensor with size being (3, 2)\n",
    "x = torch.Tensor([[1, 2], [3, 4], [5, 6]])\n",
    "\n",
    "# y is a tensor with size being (3, 2)\n",
    "y = torch.Tensor([[-1, -2], [-3, -4], [-5, -6]])\n",
    "\n",
    "# Our goal is to generate a tensor z with size as (2, 3, 2), and z[0,:,:] = x, z[1,:,:] = y.\n",
    "\n",
    "# Use torch.stack to generate such a z\n",
    "z = torch.stack((x, y))\n",
    "# print(z)\n",
    "print(z[0,:,:])\n",
    "\n",
    "# Use torch.cat and torch.unsqueeze to generate such a z\n",
    "# print('torch.unsqueeze(x, 0) = ', torch.unsqueeze(x, 0))\n",
    "z = torch.cat((torch.unsqueeze(x, 0), torch.unsqueeze(y, 0)))\n",
    "# print(z)\n",
    "print(z[1,:,:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WGw4eEo-eeHm"
   },
   "source": [
    "### 3) Tensor expansion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KAII9eJgJFK2"
   },
   "source": [
    "Tensor expansion is to expand a tensor into a larger tensor along singleton dimensions. The corresponding functions are [torch.Tensor.expand](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.expand) and [torch.Tensor.expand_as](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.expand_as). Please read the documents of the functions, and finish the following practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "sQbFte-AJzVL",
    "outputId": "0505d6ed-fcff-40b1-d373-2510a4deb373"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# x is a tensor with size being (3)\n",
    "x = torch.Tensor([1, 2, 3])\n",
    "\n",
    "# Our goal is to generate a tensor z with size (2, 3), so that z[0,:,:] = x, z[1,:,:] = x.\n",
    "\n",
    "# [TO DO]\n",
    "# Change the size of x into (1, 3) by using torch.unsqueeze.\n",
    "x = torch.unsqueeze(x, 0)\n",
    "print(x.shape)\n",
    "\n",
    "# [TO DO]\n",
    "# Then expand the new tensor to the target tensor by using torch.Tensor.expand.\n",
    "z = x.expand(2, -1)\n",
    "# print(z)\n",
    "print(z.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0rFL_Shoef3m"
   },
   "source": [
    "### 4) Tensor reduction in a given dimension"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fmEoJVw0LL9H"
   },
   "source": [
    "In deep learning, we often need to compute the mean/sum/max/min value in a given dimension of a tensor. Please read the document of [torch.mean](https://pytorch.org/docs/stable/torch.html#torch.mean), [torch.sum](https://pytorch.org/docs/stable/torch.html#torch.sum), [torch.max](https://pytorch.org/docs/stable/torch.html#torch.max), [torch.min](https://pytorch.org/docs/stable/torch.html#torch.min), [torch.topk](https://pytorch.org/docs/stable/torch.html#torch.topk), and finish the following practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "A7dlZwe4MNxo",
    "outputId": "1148ec88-6f73-4cdc-8471-40d431ca429b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2702)\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 5])\n"
     ]
    }
   ],
   "source": [
    "# x is a random tensor with size being (10, 50)\n",
    "x = torch.randn(10, 50)\n",
    "\n",
    "# Compute the mean value for each row of x.\n",
    "# You need to generate a tensor x_mean of size (10), and x_mean[k, :] is the mean value of the k-th row of x.\n",
    "\n",
    "# dim = 1: eliminate the second(1)'s dimension\n",
    "\n",
    "x_mean = torch.mean(x, dim=1)\n",
    "# print(x_mean)\n",
    "print(x_mean[3, ])\n",
    "\n",
    "# Compute the sum value for each row of x.\n",
    "# You need to generate a tensor x_sum of size (10).\n",
    "x_sum = torch.sum(x, dim=1)\n",
    "print(x_sum.shape)\n",
    "\n",
    "# Compute the max value for each row of x.\n",
    "# You need to generate a tensor x_max of size (10).\n",
    "(x_max, indices) = torch.max(x, dim=1)\n",
    "# print(x_max, indices)\n",
    "print(x_max.shape)\n",
    "\n",
    "# Compute the min value for each row of x.\n",
    "# You need to generate a tensor x_min of size (10).\n",
    "(x_min, indices) = torch.min(x, dim=1)\n",
    "print(x_min.shape)\n",
    "\n",
    "# Compute the top-5 values for each row of x.\n",
    "# (wrong) You need to generate a tensor x_mean of size (10, 5), and x_top[k, :] is the top-5 values of each row in x.\n",
    "# (right) You need to generate a tensor, top-5 values of each row\n",
    "(x_xtop, indices) = torch.topk(x, k=5, dim=1)\n",
    "print((x_xtop.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I49qjiqHB9oa"
   },
   "source": [
    "## Convolutional Neural Networks\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JePbG5pSt1xv"
   },
   "source": [
    "Implement a convolutional neural network for image classification on CIFAR-10 dataset.\n",
    "\n",
    "CIFAR-10 is an image dataset of 10 categories. Each image has a size of 32x32 pixels. The following code will download the dataset, and split it into `train` and `test`. For this question, we use the default validation split generated by Skorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120,
     "referenced_widgets": [
      "310efd92386a4757a767d5204b08a865",
      "8346dadd8dcd437f9d0c73a8e86d4503",
      "87cad6ba7ef84d03a7f8c0cb37ec01ec",
      "b712d5d1ba464a378c00dbb2aec6dd8c",
      "0f3b4a3b037d47cb8ba1196d7dbd3b98",
      "3f3ef2c146bf4dd38424170b3090b055",
      "adbb9a799ba44a91b927b44d0c27e6d1",
      "44a89539615b4132a01ca7f5223ca281"
     ]
    },
    "colab_type": "code",
    "id": "sQxOUQ29BuMB",
    "outputId": "fe539e24-4cd1-42ef-8bda-9ec1a0accd54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train = torchvision.datasets.CIFAR10(\"./data\", train=True, download=True)\n",
    "test = torchvision.datasets.CIFAR10(\"./data\", train=False, download=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ieBpiwMwi6wD"
   },
   "source": [
    "The following code visualizes some samples in the dataset. You may use it to debug your model if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "cU5HrxybupyJ",
    "outputId": "c9568768-b950-40b7-c16a-9d21a0969075"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot(data, labels=None, num_sample=5):\n",
    "  n = min(len(data), num_sample)\n",
    "  for i in range(n):\n",
    "    plt.subplot(1, n, i+1)\n",
    "    plt.imshow(data[i], cmap=\"gray\")\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    if labels is not None:\n",
    "      plt.title(labels[i])\n",
    "\n",
    "train.labels = [train.classes[target] for target in train.targets]\n",
    "plot(train.data, train.labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JwzKmdcuCv1D"
   },
   "source": [
    "### 1) Basic CNN implementation\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TbEYo5WgjTtm"
   },
   "source": [
    "Consider a basic CNN model\n",
    "\n",
    "- It has 3 convolutional layers, followed by a linear layer.\n",
    "- Each convolutional layer has a kernel size of 3, a padding of 1.\n",
    "- ReLU activation is applied on every hidden layer.\n",
    "\n",
    "Please implement this model in the following section. The hyperparameters is then be tuned and you need to fill the results in the table."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OZKyE2GUfL-Z"
   },
   "source": [
    "#### a) Implement convolutional layers (10 Points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4P_aYytExtq9"
   },
   "source": [
    "Implement the initialization function and the forward function of the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sDmCKUD1LBFk"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "  def __init__(self, channels):\n",
    "    super(CNN, self).__init__()\n",
    "    # implement parameter definitions here\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "    # print('channels = ', channels)\n",
    "    self.channels = channels\n",
    "\n",
    "    self.backbone = nn.Sequential(\n",
    "        nn.Conv2d(3, channels, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "\n",
    "        nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "\n",
    "        nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "\n",
    "    # regard the image has the same size 32 * 32\n",
    "    self.f1 = nn.Linear(channels * 32 * 32, 10)\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "  \n",
    "  def forward(self, images):\n",
    "    # implement the forward function here\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    images = self.backbone(images)\n",
    "\n",
    "    # flatten the image\n",
    "    images = images.view(images.shape[0], -1)\n",
    "    images = self.f1(images)\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E_YaASPpgRiL"
   },
   "source": [
    "#### b) Tune hyperparameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ygMcDdpy6XWP"
   },
   "source": [
    "Train the CNN model on CIFAR-10 dataset. We can tune the number of channels, optimizer, learning rate and the number of epochs for best validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "JUaguxFA5xOp",
    "outputId": "e74d78b8-10c0-414a-93d4-128f8ca4b2fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The channel was 128, the learning rate was 0.001 and the optimizer was <class 'torch.optim.sgd.SGD'>\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m2.1158\u001b[0m       \u001b[32m0.3160\u001b[0m        \u001b[35m1.9293\u001b[0m  8.0129\n",
      "      2        \u001b[36m1.8676\u001b[0m       \u001b[32m0.3645\u001b[0m        \u001b[35m1.8060\u001b[0m  7.0036\n",
      "      3        \u001b[36m1.7704\u001b[0m       \u001b[32m0.3984\u001b[0m        \u001b[35m1.7152\u001b[0m  7.0300\n",
      "      4        \u001b[36m1.6917\u001b[0m       \u001b[32m0.4253\u001b[0m        \u001b[35m1.6462\u001b[0m  7.0520\n",
      "      5        \u001b[36m1.6336\u001b[0m       \u001b[32m0.4405\u001b[0m        \u001b[35m1.5969\u001b[0m  7.0703\n",
      "      6        \u001b[36m1.5857\u001b[0m       \u001b[32m0.4537\u001b[0m        \u001b[35m1.5550\u001b[0m  7.0006\n",
      "      7        \u001b[36m1.5434\u001b[0m       \u001b[32m0.4653\u001b[0m        \u001b[35m1.5178\u001b[0m  7.0303\n",
      "      8        \u001b[36m1.5062\u001b[0m       \u001b[32m0.4777\u001b[0m        \u001b[35m1.4855\u001b[0m  6.9867\n",
      "      9        \u001b[36m1.4740\u001b[0m       \u001b[32m0.4851\u001b[0m        \u001b[35m1.4584\u001b[0m  7.0018\n",
      "     10        \u001b[36m1.4458\u001b[0m       \u001b[32m0.4927\u001b[0m        \u001b[35m1.4353\u001b[0m  7.0111\n",
      "     11        \u001b[36m1.4201\u001b[0m       \u001b[32m0.4999\u001b[0m        \u001b[35m1.4149\u001b[0m  7.0238\n",
      "     12        \u001b[36m1.3961\u001b[0m       \u001b[32m0.5051\u001b[0m        \u001b[35m1.3962\u001b[0m  7.0131\n",
      "     13        \u001b[36m1.3729\u001b[0m       \u001b[32m0.5109\u001b[0m        \u001b[35m1.3787\u001b[0m  7.0701\n",
      "     14        \u001b[36m1.3496\u001b[0m       \u001b[32m0.5166\u001b[0m        \u001b[35m1.3605\u001b[0m  7.0898\n",
      "     15        \u001b[36m1.3254\u001b[0m       \u001b[32m0.5228\u001b[0m        \u001b[35m1.3404\u001b[0m  7.0641\n",
      "     16        \u001b[36m1.3003\u001b[0m       \u001b[32m0.5288\u001b[0m        \u001b[35m1.3189\u001b[0m  7.0448\n",
      "     17        \u001b[36m1.2751\u001b[0m       \u001b[32m0.5329\u001b[0m        \u001b[35m1.2981\u001b[0m  7.0350\n",
      "     18        \u001b[36m1.2510\u001b[0m       \u001b[32m0.5411\u001b[0m        \u001b[35m1.2792\u001b[0m  7.0506\n",
      "     19        \u001b[36m1.2286\u001b[0m       \u001b[32m0.5472\u001b[0m        \u001b[35m1.2625\u001b[0m  7.0371\n",
      "     20        \u001b[36m1.2082\u001b[0m       \u001b[32m0.5523\u001b[0m        \u001b[35m1.2481\u001b[0m  7.0452\n",
      "     21        \u001b[36m1.1893\u001b[0m       \u001b[32m0.5558\u001b[0m        \u001b[35m1.2354\u001b[0m  7.0610\n",
      "     22        \u001b[36m1.1714\u001b[0m       \u001b[32m0.5615\u001b[0m        \u001b[35m1.2241\u001b[0m  7.0859\n",
      "     23        \u001b[36m1.1543\u001b[0m       \u001b[32m0.5647\u001b[0m        \u001b[35m1.2139\u001b[0m  7.0981\n",
      "     24        \u001b[36m1.1376\u001b[0m       \u001b[32m0.5670\u001b[0m        \u001b[35m1.2042\u001b[0m  7.1177\n",
      "     25        \u001b[36m1.1213\u001b[0m       \u001b[32m0.5722\u001b[0m        \u001b[35m1.1951\u001b[0m  7.1225\n",
      "     26        \u001b[36m1.1052\u001b[0m       \u001b[32m0.5741\u001b[0m        \u001b[35m1.1863\u001b[0m  7.1420\n",
      "     27        \u001b[36m1.0892\u001b[0m       \u001b[32m0.5781\u001b[0m        \u001b[35m1.1781\u001b[0m  7.1389\n",
      "     28        \u001b[36m1.0733\u001b[0m       \u001b[32m0.5832\u001b[0m        \u001b[35m1.1701\u001b[0m  7.1626\n",
      "     29        \u001b[36m1.0574\u001b[0m       \u001b[32m0.5866\u001b[0m        \u001b[35m1.1624\u001b[0m  7.1693\n",
      "     30        \u001b[36m1.0415\u001b[0m       \u001b[32m0.5895\u001b[0m        \u001b[35m1.1553\u001b[0m  7.1629\n",
      "     31        \u001b[36m1.0258\u001b[0m       \u001b[32m0.5915\u001b[0m        \u001b[35m1.1485\u001b[0m  7.1485\n",
      "     32        \u001b[36m1.0102\u001b[0m       \u001b[32m0.5954\u001b[0m        \u001b[35m1.1422\u001b[0m  7.1612\n",
      "     33        \u001b[36m0.9948\u001b[0m       \u001b[32m0.5981\u001b[0m        \u001b[35m1.1362\u001b[0m  7.1532\n",
      "     34        \u001b[36m0.9796\u001b[0m       \u001b[32m0.6016\u001b[0m        \u001b[35m1.1305\u001b[0m  7.1491\n",
      "     35        \u001b[36m0.9645\u001b[0m       \u001b[32m0.6052\u001b[0m        \u001b[35m1.1253\u001b[0m  7.1466\n",
      "     36        \u001b[36m0.9495\u001b[0m       \u001b[32m0.6078\u001b[0m        \u001b[35m1.1203\u001b[0m  7.1506\n",
      "     37        \u001b[36m0.9347\u001b[0m       \u001b[32m0.6089\u001b[0m        \u001b[35m1.1159\u001b[0m  7.1595\n",
      "     38        \u001b[36m0.9200\u001b[0m       \u001b[32m0.6109\u001b[0m        \u001b[35m1.1117\u001b[0m  7.1716\n",
      "     39        \u001b[36m0.9054\u001b[0m       \u001b[32m0.6129\u001b[0m        \u001b[35m1.1079\u001b[0m  7.1662\n",
      "     40        \u001b[36m0.8907\u001b[0m       \u001b[32m0.6141\u001b[0m        \u001b[35m1.1043\u001b[0m  7.1629\n",
      "     41        \u001b[36m0.8761\u001b[0m       \u001b[32m0.6160\u001b[0m        \u001b[35m1.1011\u001b[0m  7.1744\n",
      "     42        \u001b[36m0.8615\u001b[0m       \u001b[32m0.6161\u001b[0m        \u001b[35m1.0983\u001b[0m  7.1671\n",
      "     43        \u001b[36m0.8469\u001b[0m       \u001b[32m0.6193\u001b[0m        \u001b[35m1.0959\u001b[0m  7.1680\n",
      "     44        \u001b[36m0.8323\u001b[0m       \u001b[32m0.6205\u001b[0m        \u001b[35m1.0939\u001b[0m  7.1720\n",
      "     45        \u001b[36m0.8176\u001b[0m       \u001b[32m0.6211\u001b[0m        \u001b[35m1.0921\u001b[0m  7.1611\n",
      "     46        \u001b[36m0.8030\u001b[0m       \u001b[32m0.6235\u001b[0m        \u001b[35m1.0907\u001b[0m  7.1709\n",
      "     47        \u001b[36m0.7884\u001b[0m       \u001b[32m0.6242\u001b[0m        \u001b[35m1.0898\u001b[0m  7.1742\n",
      "     48        \u001b[36m0.7738\u001b[0m       \u001b[32m0.6250\u001b[0m        \u001b[35m1.0895\u001b[0m  7.1943\n",
      "     49        \u001b[36m0.7593\u001b[0m       \u001b[32m0.6255\u001b[0m        1.0895  7.1965\n",
      "     50        \u001b[36m0.7448\u001b[0m       \u001b[32m0.6266\u001b[0m        1.0902  7.1881\n",
      "The channel was 256, the learning rate was 0.001 and the optimizer was <class 'torch.optim.sgd.SGD'>\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m2.0442\u001b[0m       \u001b[32m0.3434\u001b[0m        \u001b[35m1.8783\u001b[0m  16.4868\n",
      "      2        \u001b[36m1.8054\u001b[0m       \u001b[32m0.3976\u001b[0m        \u001b[35m1.7304\u001b[0m  16.5860\n",
      "      3        \u001b[36m1.7004\u001b[0m       \u001b[32m0.4273\u001b[0m        \u001b[35m1.6449\u001b[0m  16.6578\n",
      "      4        \u001b[36m1.6279\u001b[0m       \u001b[32m0.4455\u001b[0m        \u001b[35m1.5829\u001b[0m  16.6932\n",
      "      5        \u001b[36m1.5640\u001b[0m       \u001b[32m0.4653\u001b[0m        \u001b[35m1.5236\u001b[0m  16.7557\n",
      "      6        \u001b[36m1.5044\u001b[0m       \u001b[32m0.4785\u001b[0m        \u001b[35m1.4709\u001b[0m  16.7104\n",
      "      7        \u001b[36m1.4544\u001b[0m       \u001b[32m0.4940\u001b[0m        \u001b[35m1.4321\u001b[0m  16.7861\n",
      "      8        \u001b[36m1.4143\u001b[0m       \u001b[32m0.5052\u001b[0m        \u001b[35m1.4030\u001b[0m  16.7697\n",
      "      9        \u001b[36m1.3789\u001b[0m       \u001b[32m0.5118\u001b[0m        \u001b[35m1.3771\u001b[0m  16.7234\n",
      "     10        \u001b[36m1.3442\u001b[0m       \u001b[32m0.5204\u001b[0m        \u001b[35m1.3506\u001b[0m  16.7258\n",
      "     11        \u001b[36m1.3091\u001b[0m       \u001b[32m0.5285\u001b[0m        \u001b[35m1.3241\u001b[0m  16.5999\n",
      "     12        \u001b[36m1.2755\u001b[0m       \u001b[32m0.5356\u001b[0m        \u001b[35m1.3007\u001b[0m  16.5341\n",
      "     13        \u001b[36m1.2457\u001b[0m       \u001b[32m0.5426\u001b[0m        \u001b[35m1.2806\u001b[0m  16.4736\n",
      "     14        \u001b[36m1.2194\u001b[0m       \u001b[32m0.5506\u001b[0m        \u001b[35m1.2631\u001b[0m  16.4122\n",
      "     15        \u001b[36m1.1957\u001b[0m       \u001b[32m0.5565\u001b[0m        \u001b[35m1.2472\u001b[0m  16.3709\n",
      "     16        \u001b[36m1.1736\u001b[0m       \u001b[32m0.5612\u001b[0m        \u001b[35m1.2330\u001b[0m  16.4181\n",
      "     17        \u001b[36m1.1525\u001b[0m       \u001b[32m0.5658\u001b[0m        \u001b[35m1.2197\u001b[0m  16.4600\n",
      "     18        \u001b[36m1.1320\u001b[0m       \u001b[32m0.5706\u001b[0m        \u001b[35m1.2072\u001b[0m  16.4435\n",
      "     19        \u001b[36m1.1117\u001b[0m       \u001b[32m0.5745\u001b[0m        \u001b[35m1.1960\u001b[0m  16.4278\n",
      "     20        \u001b[36m1.0917\u001b[0m       \u001b[32m0.5785\u001b[0m        \u001b[35m1.1856\u001b[0m  16.4144\n",
      "     21        \u001b[36m1.0720\u001b[0m       \u001b[32m0.5834\u001b[0m        \u001b[35m1.1760\u001b[0m  16.4010\n",
      "     22        \u001b[36m1.0525\u001b[0m       \u001b[32m0.5882\u001b[0m        \u001b[35m1.1671\u001b[0m  16.3734\n",
      "     23        \u001b[36m1.0335\u001b[0m       \u001b[32m0.5924\u001b[0m        \u001b[35m1.1589\u001b[0m  16.3465\n",
      "     24        \u001b[36m1.0148\u001b[0m       \u001b[32m0.5960\u001b[0m        \u001b[35m1.1515\u001b[0m  16.3348\n",
      "     25        \u001b[36m0.9964\u001b[0m       \u001b[32m0.5974\u001b[0m        \u001b[35m1.1447\u001b[0m  16.3804\n",
      "     26        \u001b[36m0.9784\u001b[0m       \u001b[32m0.5993\u001b[0m        \u001b[35m1.1385\u001b[0m  16.3272\n",
      "     27        \u001b[36m0.9607\u001b[0m       \u001b[32m0.6022\u001b[0m        \u001b[35m1.1328\u001b[0m  16.3608\n",
      "     28        \u001b[36m0.9431\u001b[0m       \u001b[32m0.6049\u001b[0m        \u001b[35m1.1276\u001b[0m  16.3947\n",
      "     29        \u001b[36m0.9255\u001b[0m       \u001b[32m0.6067\u001b[0m        \u001b[35m1.1225\u001b[0m  16.4668\n",
      "     30        \u001b[36m0.9080\u001b[0m       \u001b[32m0.6086\u001b[0m        \u001b[35m1.1179\u001b[0m  16.5375\n",
      "     31        \u001b[36m0.8904\u001b[0m       0.6086        \u001b[35m1.1137\u001b[0m  16.5316\n",
      "     32        \u001b[36m0.8726\u001b[0m       \u001b[32m0.6107\u001b[0m        \u001b[35m1.1097\u001b[0m  16.6019\n",
      "     33        \u001b[36m0.8547\u001b[0m       \u001b[32m0.6129\u001b[0m        \u001b[35m1.1059\u001b[0m  16.6545\n",
      "     34        \u001b[36m0.8366\u001b[0m       \u001b[32m0.6152\u001b[0m        \u001b[35m1.1026\u001b[0m  16.6232\n",
      "     35        \u001b[36m0.8184\u001b[0m       \u001b[32m0.6159\u001b[0m        \u001b[35m1.0996\u001b[0m  16.6570\n",
      "     36        \u001b[36m0.8000\u001b[0m       \u001b[32m0.6172\u001b[0m        \u001b[35m1.0973\u001b[0m  16.5988\n",
      "     37        \u001b[36m0.7814\u001b[0m       \u001b[32m0.6189\u001b[0m        \u001b[35m1.0957\u001b[0m  16.5260\n",
      "     38        \u001b[36m0.7628\u001b[0m       \u001b[32m0.6214\u001b[0m        \u001b[35m1.0950\u001b[0m  16.5240\n",
      "     39        \u001b[36m0.7441\u001b[0m       \u001b[32m0.6227\u001b[0m        1.0952  16.4719\n",
      "     40        \u001b[36m0.7254\u001b[0m       0.6224        1.0965  16.4138\n",
      "     41        \u001b[36m0.7067\u001b[0m       0.6222        1.0988  16.4675\n",
      "     42        \u001b[36m0.6880\u001b[0m       0.6223        1.1025  16.4783\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "The channel was 512, the learning rate was 0.001 and the optimizer was <class 'torch.optim.sgd.SGD'>\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m1.9867\u001b[0m       \u001b[32m0.3623\u001b[0m        \u001b[35m1.8191\u001b[0m  46.1175\n",
      "      2        \u001b[36m1.7513\u001b[0m       \u001b[32m0.4171\u001b[0m        \u001b[35m1.6693\u001b[0m  46.1142\n",
      "      3        \u001b[36m1.6400\u001b[0m       \u001b[32m0.4468\u001b[0m        \u001b[35m1.5838\u001b[0m  45.7091\n",
      "      4        \u001b[36m1.5603\u001b[0m       \u001b[32m0.4680\u001b[0m        \u001b[35m1.5144\u001b[0m  45.3723\n",
      "      5        \u001b[36m1.4935\u001b[0m       \u001b[32m0.4853\u001b[0m        \u001b[35m1.4582\u001b[0m  45.4629\n",
      "      6        \u001b[36m1.4396\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m1.4170\u001b[0m  45.4543\n",
      "      7        \u001b[36m1.3952\u001b[0m       \u001b[32m0.5077\u001b[0m        \u001b[35m1.3844\u001b[0m  45.3653\n",
      "      8        \u001b[36m1.3541\u001b[0m       \u001b[32m0.5188\u001b[0m        \u001b[35m1.3529\u001b[0m  45.4058\n",
      "      9        \u001b[36m1.3127\u001b[0m       \u001b[32m0.5287\u001b[0m        \u001b[35m1.3208\u001b[0m  45.8955\n",
      "     10        \u001b[36m1.2729\u001b[0m       \u001b[32m0.5375\u001b[0m        \u001b[35m1.2915\u001b[0m  46.3144\n",
      "     11        \u001b[36m1.2376\u001b[0m       \u001b[32m0.5454\u001b[0m        \u001b[35m1.2668\u001b[0m  46.4242\n",
      "     12        \u001b[36m1.2072\u001b[0m       \u001b[32m0.5545\u001b[0m        \u001b[35m1.2464\u001b[0m  46.3143\n",
      "     13        \u001b[36m1.1800\u001b[0m       \u001b[32m0.5601\u001b[0m        \u001b[35m1.2289\u001b[0m  45.7325\n",
      "     14        \u001b[36m1.1545\u001b[0m       \u001b[32m0.5658\u001b[0m        \u001b[35m1.2133\u001b[0m  45.9819\n",
      "     15        \u001b[36m1.1300\u001b[0m       \u001b[32m0.5724\u001b[0m        \u001b[35m1.1989\u001b[0m  45.9304\n",
      "     16        \u001b[36m1.1059\u001b[0m       \u001b[32m0.5773\u001b[0m        \u001b[35m1.1857\u001b[0m  45.4933\n",
      "     17        \u001b[36m1.0821\u001b[0m       \u001b[32m0.5817\u001b[0m        \u001b[35m1.1732\u001b[0m  45.3715\n",
      "     18        \u001b[36m1.0587\u001b[0m       \u001b[32m0.5876\u001b[0m        \u001b[35m1.1618\u001b[0m  45.4426\n",
      "     19        \u001b[36m1.0358\u001b[0m       \u001b[32m0.5915\u001b[0m        \u001b[35m1.1513\u001b[0m  45.5602\n",
      "     20        \u001b[36m1.0134\u001b[0m       \u001b[32m0.5973\u001b[0m        \u001b[35m1.1418\u001b[0m  46.1363\n",
      "     21        \u001b[36m0.9915\u001b[0m       \u001b[32m0.6005\u001b[0m        \u001b[35m1.1330\u001b[0m  46.3644\n",
      "     22        \u001b[36m0.9700\u001b[0m       \u001b[32m0.6039\u001b[0m        \u001b[35m1.1250\u001b[0m  45.7330\n",
      "     23        \u001b[36m0.9487\u001b[0m       \u001b[32m0.6072\u001b[0m        \u001b[35m1.1174\u001b[0m  45.8170\n",
      "     24        \u001b[36m0.9277\u001b[0m       \u001b[32m0.6107\u001b[0m        \u001b[35m1.1103\u001b[0m  45.4220\n",
      "     25        \u001b[36m0.9067\u001b[0m       \u001b[32m0.6116\u001b[0m        \u001b[35m1.1037\u001b[0m  45.4124\n",
      "     26        \u001b[36m0.8858\u001b[0m       \u001b[32m0.6138\u001b[0m        \u001b[35m1.0975\u001b[0m  45.3856\n",
      "     27        \u001b[36m0.8651\u001b[0m       \u001b[32m0.6175\u001b[0m        \u001b[35m1.0919\u001b[0m  45.7772\n",
      "     28        \u001b[36m0.8444\u001b[0m       \u001b[32m0.6187\u001b[0m        \u001b[35m1.0870\u001b[0m  46.3898\n",
      "     29        \u001b[36m0.8239\u001b[0m       \u001b[32m0.6195\u001b[0m        \u001b[35m1.0828\u001b[0m  46.3944\n",
      "     30        \u001b[36m0.8035\u001b[0m       \u001b[32m0.6216\u001b[0m        \u001b[35m1.0797\u001b[0m  46.1950\n",
      "     31        \u001b[36m0.7833\u001b[0m       \u001b[32m0.6235\u001b[0m        \u001b[35m1.0776\u001b[0m  45.4395\n",
      "     32        \u001b[36m0.7632\u001b[0m       \u001b[32m0.6250\u001b[0m        \u001b[35m1.0765\u001b[0m  45.5423\n",
      "     33        \u001b[36m0.7433\u001b[0m       \u001b[32m0.6255\u001b[0m        1.0766  45.4350\n",
      "     34        \u001b[36m0.7235\u001b[0m       \u001b[32m0.6264\u001b[0m        1.0779  45.3818\n",
      "     35        \u001b[36m0.7037\u001b[0m       \u001b[32m0.6283\u001b[0m        1.0803  45.3848\n",
      "     36        \u001b[36m0.6839\u001b[0m       \u001b[32m0.6295\u001b[0m        1.0839  45.7210\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "The channel was 128, the learning rate was 0.001 and the optimizer was <class 'torch.optim.adam.Adam'>\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.4839\u001b[0m       \u001b[32m0.5829\u001b[0m        \u001b[35m1.1889\u001b[0m  7.5850\n",
      "      2        \u001b[36m1.0473\u001b[0m       \u001b[32m0.6502\u001b[0m        \u001b[35m1.0072\u001b[0m  7.5615\n",
      "      3        \u001b[36m0.8172\u001b[0m       0.6484        1.0422  7.5612\n",
      "      4        \u001b[36m0.6176\u001b[0m       0.6250        1.2363  7.5515\n",
      "      5        \u001b[36m0.4350\u001b[0m       0.5978        1.5414  7.5729\n",
      "      6        \u001b[36m0.3217\u001b[0m       0.6011        1.8818  7.5781\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "The channel was 256, the learning rate was 0.001 and the optimizer was <class 'torch.optim.adam.Adam'>\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m1.7182\u001b[0m       \u001b[32m0.5077\u001b[0m        \u001b[35m1.3801\u001b[0m  16.8513\n",
      "      2        \u001b[36m1.2644\u001b[0m       \u001b[32m0.5565\u001b[0m        \u001b[35m1.2567\u001b[0m  16.8689\n",
      "      3        \u001b[36m1.0805\u001b[0m       \u001b[32m0.5825\u001b[0m        \u001b[35m1.1950\u001b[0m  16.9014\n",
      "      4        \u001b[36m0.8733\u001b[0m       0.5819        1.2774  16.8458\n",
      "      5        \u001b[36m0.6655\u001b[0m       0.5767        1.5078  16.8153\n",
      "      6        \u001b[36m0.5032\u001b[0m       0.5598        1.8750  16.8164\n",
      "      7        \u001b[36m0.3858\u001b[0m       0.5273        2.4469  16.8811\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "The channel was 512, the learning rate was 0.001 and the optimizer was <class 'torch.optim.adam.Adam'>\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m2.3566\u001b[0m       \u001b[32m0.1000\u001b[0m        \u001b[35m2.3026\u001b[0m  46.3657\n",
      "      2        \u001b[36m2.3028\u001b[0m       0.1000        2.3026  46.2071\n",
      "      3        2.3028       0.1000        2.3026  45.8301\n",
      "      4        2.3028       0.1000        2.3026  46.2545\n",
      "      5        2.3028       0.1000        2.3026  46.7461\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "The channel was 128, the learning rate was 0.01 and the optimizer was <class 'torch.optim.sgd.SGD'>\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.7802\u001b[0m       \u001b[32m0.4473\u001b[0m        \u001b[35m1.5509\u001b[0m  7.2227\n",
      "      2        \u001b[36m1.4604\u001b[0m       \u001b[32m0.4953\u001b[0m        \u001b[35m1.3926\u001b[0m  7.1891\n",
      "      3        \u001b[36m1.3259\u001b[0m       \u001b[32m0.5312\u001b[0m        \u001b[35m1.2938\u001b[0m  7.1699\n",
      "      4        \u001b[36m1.2092\u001b[0m       \u001b[32m0.5520\u001b[0m        \u001b[35m1.2376\u001b[0m  7.1683\n",
      "      5        \u001b[36m1.1174\u001b[0m       \u001b[32m0.5713\u001b[0m        \u001b[35m1.1887\u001b[0m  7.1695\n",
      "      6        \u001b[36m1.0313\u001b[0m       \u001b[32m0.5887\u001b[0m        \u001b[35m1.1468\u001b[0m  7.1558\n",
      "      7        \u001b[36m0.9441\u001b[0m       \u001b[32m0.6095\u001b[0m        \u001b[35m1.1049\u001b[0m  7.1596\n",
      "      8        \u001b[36m0.8574\u001b[0m       \u001b[32m0.6212\u001b[0m        \u001b[35m1.0799\u001b[0m  7.1328\n",
      "      9        \u001b[36m0.7744\u001b[0m       \u001b[32m0.6294\u001b[0m        1.0816  7.1242\n",
      "     10        \u001b[36m0.6936\u001b[0m       \u001b[32m0.6324\u001b[0m        1.1068  7.1195\n",
      "     11        \u001b[36m0.6121\u001b[0m       0.6323        1.1574  7.1237\n",
      "     12        \u001b[36m0.5278\u001b[0m       0.6290        1.2431  7.1241\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "The channel was 256, the learning rate was 0.01 and the optimizer was <class 'torch.optim.sgd.SGD'>\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m1.7304\u001b[0m       \u001b[32m0.4675\u001b[0m        \u001b[35m1.4906\u001b[0m  16.3829\n",
      "      2        \u001b[36m1.3863\u001b[0m       \u001b[32m0.5328\u001b[0m        \u001b[35m1.3044\u001b[0m  16.5476\n",
      "      3        \u001b[36m1.2254\u001b[0m       \u001b[32m0.5659\u001b[0m        \u001b[35m1.2175\u001b[0m  16.5300\n",
      "      4        \u001b[36m1.1000\u001b[0m       \u001b[32m0.5942\u001b[0m        \u001b[35m1.1371\u001b[0m  16.5410\n",
      "      5        \u001b[36m0.9843\u001b[0m       \u001b[32m0.6187\u001b[0m        \u001b[35m1.0781\u001b[0m  16.5368\n",
      "      6        \u001b[36m0.8811\u001b[0m       \u001b[32m0.6341\u001b[0m        \u001b[35m1.0504\u001b[0m  16.4840\n",
      "      7        \u001b[36m0.7862\u001b[0m       \u001b[32m0.6431\u001b[0m        \u001b[35m1.0436\u001b[0m  16.4648\n",
      "      8        \u001b[36m0.6937\u001b[0m       \u001b[32m0.6450\u001b[0m        1.0633  16.3988\n",
      "      9        \u001b[36m0.5998\u001b[0m       0.6420        1.1201  16.3038\n",
      "     10        \u001b[36m0.5008\u001b[0m       0.6338        1.2165  16.3112\n",
      "     11        \u001b[36m0.3968\u001b[0m       0.6297        1.3476  16.3209\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "The channel was 512, the learning rate was 0.01 and the optimizer was <class 'torch.optim.sgd.SGD'>\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m1.7122\u001b[0m       \u001b[32m0.4814\u001b[0m        \u001b[35m1.4455\u001b[0m  45.7170\n",
      "      2        \u001b[36m1.3445\u001b[0m       \u001b[32m0.5414\u001b[0m        \u001b[35m1.2749\u001b[0m  45.8045\n",
      "      3        \u001b[36m1.1590\u001b[0m       \u001b[32m0.5824\u001b[0m        \u001b[35m1.1645\u001b[0m  46.0595\n",
      "      4        \u001b[36m1.0109\u001b[0m       \u001b[32m0.6085\u001b[0m        \u001b[35m1.0983\u001b[0m  46.3534\n",
      "      5        \u001b[36m0.8835\u001b[0m       \u001b[32m0.6283\u001b[0m        \u001b[35m1.0726\u001b[0m  46.3809\n",
      "      6        \u001b[36m0.7679\u001b[0m       \u001b[32m0.6391\u001b[0m        1.0765  46.4801\n",
      "      7        \u001b[36m0.6554\u001b[0m       0.6344        1.1211  46.4627\n",
      "      8        \u001b[36m0.5380\u001b[0m       0.6302        1.2184  45.9412\n",
      "      9        \u001b[36m0.4127\u001b[0m       0.6206        1.3871  45.4630\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "The channel was 128, the learning rate was 0.01 and the optimizer was <class 'torch.optim.adam.Adam'>\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m2.9942\u001b[0m       \u001b[32m0.1000\u001b[0m        \u001b[35m2.3039\u001b[0m  7.4479\n",
      "      2        \u001b[36m2.3042\u001b[0m       0.1000        2.3039  7.4428\n",
      "      3        2.3042       0.1000        2.3039  7.4505\n",
      "      4        2.3042       0.1000        2.3039  7.4814\n",
      "      5        2.3042       0.1000        2.3039  7.4697\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "The channel was 256, the learning rate was 0.01 and the optimizer was <class 'torch.optim.adam.Adam'>\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m8.8825\u001b[0m       \u001b[32m0.1000\u001b[0m        \u001b[35m2.3039\u001b[0m  16.5144\n",
      "      2        \u001b[36m2.3042\u001b[0m       0.1000        2.3039  16.7190\n",
      "      3        2.3042       0.1000        2.3039  16.8494\n",
      "      4        2.3042       0.1000        2.3039  16.9722\n",
      "      5        2.3042       0.1000        2.3039  17.0036\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "The channel was 512, the learning rate was 0.01 and the optimizer was <class 'torch.optim.adam.Adam'>\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1       \u001b[36m55.4451\u001b[0m       \u001b[32m0.1000\u001b[0m        \u001b[35m2.3038\u001b[0m  46.9036\n",
      "      2        \u001b[36m2.3042\u001b[0m       0.1000        2.3039  46.8506\n",
      "      3        2.3042       0.1000        2.3039  46.5957\n",
      "      4        2.3042       0.1000        2.3039  45.6649\n",
      "      5        2.3042       0.1000        2.3039  45.5824\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n"
     ]
    }
   ],
   "source": [
    "# implement hyperparameters, you can select and modify the hyperparameters by yourself here.\n",
    "\n",
    "optimizer = [torch.optim.SGD, torch.optim.Adam]\n",
    "learning_rate = [1e-3, 1e-2]\n",
    "channel = [128, 256, 512]\n",
    "\n",
    "train_data_normalized = torch.Tensor(train.data/255)\n",
    "train_data_normalized = train_data_normalized.permute(0,3,1,2)\n",
    "\n",
    "for l in learning_rate:\n",
    "  for o in optimizer: \n",
    "    for c in channel:\n",
    "      print(f'The channel was {c}, the learning rate was {l} and the optimizer was {str(o)}')\n",
    "\n",
    "      cnn = CNN(channels = c)\n",
    "      \n",
    "      model = skorch.NeuralNetClassifier(cnn, criterion=torch.nn.CrossEntropyLoss,\n",
    "                                   device=\"cuda\",\n",
    "                                   optimizer=o,\n",
    "                                  #  optimizer__momentum=0.90,\n",
    "                                   lr=l,\n",
    "                                   max_epochs=50,\n",
    "                                   batch_size=32,\n",
    "                                   callbacks=[skorch.callbacks.EarlyStopping(lower_is_better=True)])\n",
    "      # implement input normalization & type cast here \n",
    "      model.fit(train_data_normalized, np.asarray(train.targets))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w-EHKzozkRbD"
   },
   "source": [
    "Write down **validation accuracy** of your model under different hyperparameter settings. Note the validation set is automatically split by Skorch during `model.fit()`.\n",
    "\n",
    "learning rate = $10^{-3}$:\n",
    "| #channel for each layer \\ optimizer |  SGD   |  Adam  |\n",
    "|:-----------------------------------:|:------:|:------:|\n",
    "|                $128$                |$0.6266$|**$0.6502$**|\n",
    "|                $256$                |$0.6227$|$0.5825$|\n",
    "|                $512$                |$0.6295$|$0.1000$|\n",
    "\n",
    "learning rate = $10^{-2}$:\n",
    "| #channel for each layer \\ optimizer |  SGD   |  Adam  |\n",
    "|:-----------------------------------:|:------:|:------:|\n",
    "|                $128$                |$0.6324$|$0.1000$|\n",
    "|                $256$                |$0.6450$|$0.1000$|\n",
    "|                $512$                |$0.6391$|$0.1000$|\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "go55LVSJd-vG"
   },
   "source": [
    "### 2) Full CNN implementation (10 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6G0eCj6OmOEE"
   },
   "source": [
    "Based on the CNN in the previous question, implement a full CNN model with max pooling layer.\n",
    "\n",
    "- Add a max pooling layer after each convolutional layer.\n",
    "- Each max pooling layer has a kernel size of 2 and a stride of 2.\n",
    "\n",
    "Please implement this model in the following section. The hyperparameters is then be tuned and fill the results in the table. You are also required to complete the questions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bMrKGlMQhCa0"
   },
   "source": [
    "#### a) Implement max pooling layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g2INt6P3Myd1"
   },
   "source": [
    "Similar to the CNN implementation in previous question, implement max pooling layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DHu3Ic2dM1S9"
   },
   "outputs": [],
   "source": [
    "class CNN_MaxPool(nn.Module):\n",
    "  def __init__(self, channels):\n",
    "    super(CNN_MaxPool, self).__init__()\n",
    "    # implement parameter definitions here\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    self.channels = channels\n",
    "\n",
    "    self.backbone = nn.Sequential(\n",
    "        nn.Conv2d(3, channels, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(channels),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        nn.Dropout2d(p=0.2),\n",
    "\n",
    "        nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(channels),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        nn.Dropout2d(p=0.2),\n",
    "\n",
    "        nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(channels),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        nn.Dropout2d(p=0.2)\n",
    "    )\n",
    "\n",
    "    # regard the image has the same size 32 * 32\n",
    "    # after 3 max pooling, the size is 32 / 2 / 2 / 2 = 4\n",
    "    self.f1 = nn.Linear(channels * 4 * 4, 10)\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "  \n",
    "  def forward(self, images):\n",
    "    # implement the forward function here\n",
    "    \n",
    "    images = self.backbone(images)\n",
    "    \n",
    "    # flatten the image\n",
    "    images = images.view(images.shape[0], -1)\n",
    "\n",
    "    images = self.f1(images)\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-A6AEOoigq68"
   },
   "source": [
    "#### b) Tune hyperparameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "drH4MHSVNqwz"
   },
   "source": [
    "Based on the better optimizer found in the previous problem, we can tune the number of channels and learning rate for best validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "colab_type": "code",
    "id": "7povzg-4Nhrr",
    "outputId": "edff26c7-6f98-49be-cac4-9e6ef53c83c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The channel was 128, the learning rate was 0.0001\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.8045\u001b[0m       \u001b[32m0.4908\u001b[0m        \u001b[35m1.4717\u001b[0m  3.2317\n",
      "      2        \u001b[36m1.4935\u001b[0m       \u001b[32m0.5442\u001b[0m        \u001b[35m1.3015\u001b[0m  3.1580\n",
      "      3        \u001b[36m1.3585\u001b[0m       \u001b[32m0.5886\u001b[0m        \u001b[35m1.1966\u001b[0m  3.1443\n",
      "      4        \u001b[36m1.2596\u001b[0m       \u001b[32m0.6146\u001b[0m        \u001b[35m1.1193\u001b[0m  3.1602\n",
      "      5        \u001b[36m1.1847\u001b[0m       \u001b[32m0.6342\u001b[0m        \u001b[35m1.0625\u001b[0m  3.1622\n",
      "      6        \u001b[36m1.1234\u001b[0m       \u001b[32m0.6518\u001b[0m        \u001b[35m1.0135\u001b[0m  3.1547\n",
      "      7        \u001b[36m1.0764\u001b[0m       \u001b[32m0.6625\u001b[0m        \u001b[35m0.9778\u001b[0m  3.1650\n",
      "      8        \u001b[36m1.0358\u001b[0m       \u001b[32m0.6719\u001b[0m        \u001b[35m0.9484\u001b[0m  3.1638\n",
      "      9        \u001b[36m0.9967\u001b[0m       \u001b[32m0.6813\u001b[0m        \u001b[35m0.9210\u001b[0m  3.1614\n",
      "     10        \u001b[36m0.9698\u001b[0m       \u001b[32m0.6923\u001b[0m        \u001b[35m0.8992\u001b[0m  3.1662\n",
      "     11        \u001b[36m0.9418\u001b[0m       \u001b[32m0.6989\u001b[0m        \u001b[35m0.8795\u001b[0m  3.1710\n",
      "     12        \u001b[36m0.9159\u001b[0m       \u001b[32m0.7008\u001b[0m        \u001b[35m0.8661\u001b[0m  3.1699\n",
      "     13        \u001b[36m0.8987\u001b[0m       \u001b[32m0.7088\u001b[0m        \u001b[35m0.8503\u001b[0m  3.1751\n",
      "     14        \u001b[36m0.8736\u001b[0m       \u001b[32m0.7155\u001b[0m        \u001b[35m0.8327\u001b[0m  3.1795\n",
      "     15        \u001b[36m0.8550\u001b[0m       \u001b[32m0.7192\u001b[0m        \u001b[35m0.8209\u001b[0m  3.1766\n",
      "     16        \u001b[36m0.8351\u001b[0m       \u001b[32m0.7245\u001b[0m        \u001b[35m0.8086\u001b[0m  3.1782\n",
      "     17        \u001b[36m0.8160\u001b[0m       \u001b[32m0.7282\u001b[0m        \u001b[35m0.7994\u001b[0m  3.1833\n",
      "     18        \u001b[36m0.8006\u001b[0m       \u001b[32m0.7312\u001b[0m        \u001b[35m0.7842\u001b[0m  3.1876\n",
      "     19        \u001b[36m0.7876\u001b[0m       \u001b[32m0.7328\u001b[0m        \u001b[35m0.7803\u001b[0m  3.1829\n",
      "     20        \u001b[36m0.7678\u001b[0m       \u001b[32m0.7339\u001b[0m        \u001b[35m0.7716\u001b[0m  3.1953\n",
      "     21        \u001b[36m0.7583\u001b[0m       \u001b[32m0.7398\u001b[0m        \u001b[35m0.7576\u001b[0m  3.1882\n",
      "     22        \u001b[36m0.7469\u001b[0m       \u001b[32m0.7415\u001b[0m        \u001b[35m0.7519\u001b[0m  3.1923\n",
      "     23        \u001b[36m0.7304\u001b[0m       0.7401        \u001b[35m0.7504\u001b[0m  3.1926\n",
      "     24        \u001b[36m0.7186\u001b[0m       \u001b[32m0.7418\u001b[0m        \u001b[35m0.7470\u001b[0m  3.1946\n",
      "     25        \u001b[36m0.7094\u001b[0m       \u001b[32m0.7458\u001b[0m        \u001b[35m0.7346\u001b[0m  3.1994\n",
      "     26        \u001b[36m0.6931\u001b[0m       0.7453        0.7383  3.1950\n",
      "     27        \u001b[36m0.6847\u001b[0m       \u001b[32m0.7467\u001b[0m        \u001b[35m0.7313\u001b[0m  3.1931\n",
      "     28        \u001b[36m0.6752\u001b[0m       \u001b[32m0.7515\u001b[0m        \u001b[35m0.7170\u001b[0m  3.1928\n",
      "     29        \u001b[36m0.6627\u001b[0m       \u001b[32m0.7544\u001b[0m        \u001b[35m0.7119\u001b[0m  3.1875\n",
      "     30        \u001b[36m0.6524\u001b[0m       0.7511        0.7180  3.1824\n",
      "     31        \u001b[36m0.6418\u001b[0m       \u001b[32m0.7569\u001b[0m        \u001b[35m0.7052\u001b[0m  3.1839\n",
      "     32        \u001b[36m0.6332\u001b[0m       \u001b[32m0.7589\u001b[0m        \u001b[35m0.6995\u001b[0m  3.1801\n",
      "     33        \u001b[36m0.6226\u001b[0m       0.7579        \u001b[35m0.6945\u001b[0m  3.1792\n",
      "     34        \u001b[36m0.6115\u001b[0m       0.7575        0.6946  3.1858\n",
      "     35        \u001b[36m0.6090\u001b[0m       \u001b[32m0.7609\u001b[0m        \u001b[35m0.6869\u001b[0m  3.1795\n",
      "     36        \u001b[36m0.5965\u001b[0m       \u001b[32m0.7622\u001b[0m        \u001b[35m0.6868\u001b[0m  3.1880\n",
      "     37        \u001b[36m0.5886\u001b[0m       0.7613        \u001b[35m0.6831\u001b[0m  3.1852\n",
      "     38        \u001b[36m0.5806\u001b[0m       \u001b[32m0.7669\u001b[0m        \u001b[35m0.6744\u001b[0m  3.1756\n",
      "     39        \u001b[36m0.5691\u001b[0m       0.7656        0.6816  3.1742\n",
      "     40        \u001b[36m0.5664\u001b[0m       \u001b[32m0.7697\u001b[0m        \u001b[35m0.6742\u001b[0m  3.1783\n",
      "     41        \u001b[36m0.5545\u001b[0m       0.7657        0.6778  3.1825\n",
      "     42        \u001b[36m0.5465\u001b[0m       \u001b[32m0.7708\u001b[0m        \u001b[35m0.6708\u001b[0m  3.1714\n",
      "     43        \u001b[36m0.5400\u001b[0m       0.7692        \u001b[35m0.6658\u001b[0m  3.1728\n",
      "     44        \u001b[36m0.5343\u001b[0m       0.7663        0.6744  3.1602\n",
      "     45        \u001b[36m0.5261\u001b[0m       \u001b[32m0.7737\u001b[0m        \u001b[35m0.6608\u001b[0m  3.1719\n",
      "     46        \u001b[36m0.5252\u001b[0m       0.7717        0.6618  3.1673\n",
      "     47        \u001b[36m0.5109\u001b[0m       0.7700        0.6638  3.1695\n",
      "     48        \u001b[36m0.5050\u001b[0m       0.7716        0.6618  3.1599\n",
      "     49        \u001b[36m0.4969\u001b[0m       \u001b[32m0.7755\u001b[0m        \u001b[35m0.6571\u001b[0m  3.1588\n",
      "     50        \u001b[36m0.4920\u001b[0m       0.7733        0.6619  3.1607\n",
      "The channel was 256, the learning rate was 0.0001\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.6336\u001b[0m       \u001b[32m0.5527\u001b[0m        \u001b[35m1.2881\u001b[0m  6.1969\n",
      "      2        \u001b[36m1.2999\u001b[0m       \u001b[32m0.6045\u001b[0m        \u001b[35m1.1220\u001b[0m  6.1984\n",
      "      3        \u001b[36m1.1574\u001b[0m       \u001b[32m0.6433\u001b[0m        \u001b[35m1.0264\u001b[0m  6.2124\n",
      "      4        \u001b[36m1.0544\u001b[0m       \u001b[32m0.6692\u001b[0m        \u001b[35m0.9555\u001b[0m  6.2015\n",
      "      5        \u001b[36m0.9872\u001b[0m       \u001b[32m0.6815\u001b[0m        \u001b[35m0.9150\u001b[0m  6.2078\n",
      "      6        \u001b[36m0.9240\u001b[0m       \u001b[32m0.6962\u001b[0m        \u001b[35m0.8753\u001b[0m  6.1982\n",
      "      7        \u001b[36m0.8772\u001b[0m       \u001b[32m0.7051\u001b[0m        \u001b[35m0.8505\u001b[0m  6.2226\n",
      "      8        \u001b[36m0.8395\u001b[0m       \u001b[32m0.7134\u001b[0m        \u001b[35m0.8215\u001b[0m  6.1924\n",
      "      9        \u001b[36m0.8040\u001b[0m       \u001b[32m0.7197\u001b[0m        \u001b[35m0.7986\u001b[0m  6.1769\n",
      "     10        \u001b[36m0.7724\u001b[0m       \u001b[32m0.7305\u001b[0m        \u001b[35m0.7737\u001b[0m  6.1587\n",
      "     11        \u001b[36m0.7434\u001b[0m       \u001b[32m0.7366\u001b[0m        \u001b[35m0.7681\u001b[0m  6.1682\n",
      "     12        \u001b[36m0.7170\u001b[0m       \u001b[32m0.7445\u001b[0m        \u001b[35m0.7461\u001b[0m  6.1463\n",
      "     13        \u001b[36m0.6899\u001b[0m       \u001b[32m0.7460\u001b[0m        \u001b[35m0.7380\u001b[0m  6.1383\n",
      "     14        \u001b[36m0.6647\u001b[0m       \u001b[32m0.7519\u001b[0m        \u001b[35m0.7267\u001b[0m  6.1318\n",
      "     15        \u001b[36m0.6392\u001b[0m       \u001b[32m0.7543\u001b[0m        \u001b[35m0.7104\u001b[0m  6.1295\n",
      "     16        \u001b[36m0.6187\u001b[0m       \u001b[32m0.7574\u001b[0m        \u001b[35m0.7044\u001b[0m  6.1260\n",
      "     17        \u001b[36m0.5990\u001b[0m       \u001b[32m0.7591\u001b[0m        \u001b[35m0.6985\u001b[0m  6.1007\n",
      "     18        \u001b[36m0.5774\u001b[0m       0.7591        \u001b[35m0.6911\u001b[0m  6.0768\n",
      "     19        \u001b[36m0.5607\u001b[0m       \u001b[32m0.7651\u001b[0m        \u001b[35m0.6901\u001b[0m  6.0741\n",
      "     20        \u001b[36m0.5382\u001b[0m       \u001b[32m0.7668\u001b[0m        \u001b[35m0.6811\u001b[0m  6.1156\n",
      "     21        \u001b[36m0.5218\u001b[0m       \u001b[32m0.7682\u001b[0m        \u001b[35m0.6737\u001b[0m  6.1021\n",
      "     22        \u001b[36m0.5011\u001b[0m       \u001b[32m0.7710\u001b[0m        \u001b[35m0.6630\u001b[0m  6.1216\n",
      "     23        \u001b[36m0.4857\u001b[0m       \u001b[32m0.7717\u001b[0m        0.6631  6.0929\n",
      "     24        \u001b[36m0.4669\u001b[0m       \u001b[32m0.7751\u001b[0m        \u001b[35m0.6545\u001b[0m  6.1075\n",
      "     25        \u001b[36m0.4572\u001b[0m       \u001b[32m0.7752\u001b[0m        \u001b[35m0.6540\u001b[0m  6.1212\n",
      "     26        \u001b[36m0.4396\u001b[0m       0.7743        0.6600  6.1145\n",
      "     27        \u001b[36m0.4239\u001b[0m       \u001b[32m0.7756\u001b[0m        \u001b[35m0.6529\u001b[0m  6.1094\n",
      "     28        \u001b[36m0.4048\u001b[0m       \u001b[32m0.7788\u001b[0m        \u001b[35m0.6474\u001b[0m  6.1126\n",
      "     29        \u001b[36m0.3948\u001b[0m       0.7783        0.6507  6.1339\n",
      "     30        \u001b[36m0.3807\u001b[0m       \u001b[32m0.7789\u001b[0m        0.6526  6.1389\n",
      "     31        \u001b[36m0.3655\u001b[0m       0.7767        0.6542  6.1480\n",
      "     32        \u001b[36m0.3515\u001b[0m       \u001b[32m0.7814\u001b[0m        \u001b[35m0.6444\u001b[0m  6.1751\n",
      "     33        \u001b[36m0.3393\u001b[0m       \u001b[32m0.7840\u001b[0m        0.6455  6.1548\n",
      "     34        \u001b[36m0.3247\u001b[0m       0.7816        0.6497  6.1603\n",
      "     35        \u001b[36m0.3153\u001b[0m       0.7823        0.6487  6.1681\n",
      "     36        \u001b[36m0.3007\u001b[0m       \u001b[32m0.7860\u001b[0m        \u001b[35m0.6388\u001b[0m  6.1728\n",
      "     37        \u001b[36m0.2920\u001b[0m       0.7851        0.6449  6.1701\n",
      "     38        \u001b[36m0.2808\u001b[0m       0.7826        0.6554  6.1807\n",
      "     39        \u001b[36m0.2743\u001b[0m       0.7845        0.6524  6.1956\n",
      "     40        \u001b[36m0.2562\u001b[0m       0.7842        0.6490  6.2041\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "The channel was 512, the learning rate was 0.0001\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m1.4867\u001b[0m       \u001b[32m0.5875\u001b[0m        \u001b[35m1.1599\u001b[0m  13.7566\n",
      "      2        \u001b[36m1.1323\u001b[0m       \u001b[32m0.6544\u001b[0m        \u001b[35m0.9950\u001b[0m  13.8768\n",
      "      3        \u001b[36m0.9813\u001b[0m       \u001b[32m0.6778\u001b[0m        \u001b[35m0.9199\u001b[0m  13.9526\n",
      "      4        \u001b[36m0.8851\u001b[0m       \u001b[32m0.7036\u001b[0m        \u001b[35m0.8458\u001b[0m  14.0137\n",
      "      5        \u001b[36m0.8155\u001b[0m       \u001b[32m0.7077\u001b[0m        \u001b[35m0.8244\u001b[0m  14.0539\n",
      "      6        \u001b[36m0.7579\u001b[0m       \u001b[32m0.7197\u001b[0m        \u001b[35m0.7897\u001b[0m  14.0974\n",
      "      7        \u001b[36m0.7093\u001b[0m       \u001b[32m0.7321\u001b[0m        \u001b[35m0.7623\u001b[0m  14.0722\n",
      "      8        \u001b[36m0.6648\u001b[0m       \u001b[32m0.7351\u001b[0m        \u001b[35m0.7490\u001b[0m  14.0787\n",
      "      9        \u001b[36m0.6232\u001b[0m       \u001b[32m0.7519\u001b[0m        \u001b[35m0.7108\u001b[0m  14.0801\n",
      "     10        \u001b[36m0.5813\u001b[0m       \u001b[32m0.7579\u001b[0m        \u001b[35m0.6961\u001b[0m  13.9528\n",
      "     11        \u001b[36m0.5446\u001b[0m       \u001b[32m0.7622\u001b[0m        \u001b[35m0.6848\u001b[0m  13.9407\n",
      "     12        \u001b[36m0.5114\u001b[0m       \u001b[32m0.7663\u001b[0m        \u001b[35m0.6737\u001b[0m  14.0248\n",
      "     13        \u001b[36m0.4788\u001b[0m       0.7650        0.6776  13.9431\n",
      "     14        \u001b[36m0.4443\u001b[0m       0.7624        0.6877  14.0306\n",
      "     15        \u001b[36m0.4155\u001b[0m       \u001b[32m0.7715\u001b[0m        \u001b[35m0.6587\u001b[0m  14.0328\n",
      "     16        \u001b[36m0.3874\u001b[0m       \u001b[32m0.7740\u001b[0m        \u001b[35m0.6575\u001b[0m  13.8163\n",
      "     17        \u001b[36m0.3581\u001b[0m       0.7736        0.6607  13.7884\n",
      "     18        \u001b[36m0.3337\u001b[0m       \u001b[32m0.7801\u001b[0m        \u001b[35m0.6459\u001b[0m  13.7710\n",
      "     19        \u001b[36m0.3078\u001b[0m       0.7799        0.6515  13.7418\n",
      "     20        \u001b[36m0.2844\u001b[0m       0.7740        0.6669  13.6890\n",
      "     21        \u001b[36m0.2644\u001b[0m       0.7758        0.6733  13.6630\n",
      "     22        \u001b[36m0.2433\u001b[0m       0.7715        0.6883  13.8098\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "The channel was 128, the learning rate was 0.001\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.5918\u001b[0m       \u001b[32m0.5432\u001b[0m        \u001b[35m1.2453\u001b[0m  3.1035\n",
      "      2        \u001b[36m1.1911\u001b[0m       \u001b[32m0.6467\u001b[0m        \u001b[35m1.0016\u001b[0m  3.1075\n",
      "      3        \u001b[36m1.0245\u001b[0m       \u001b[32m0.6690\u001b[0m        \u001b[35m0.9421\u001b[0m  3.1208\n",
      "      4        \u001b[36m0.9245\u001b[0m       \u001b[32m0.7108\u001b[0m        \u001b[35m0.8356\u001b[0m  3.1018\n",
      "      5        \u001b[36m0.8580\u001b[0m       \u001b[32m0.7191\u001b[0m        \u001b[35m0.8196\u001b[0m  3.1075\n",
      "      6        \u001b[36m0.8062\u001b[0m       \u001b[32m0.7294\u001b[0m        \u001b[35m0.7835\u001b[0m  3.1001\n",
      "      7        \u001b[36m0.7627\u001b[0m       \u001b[32m0.7342\u001b[0m        \u001b[35m0.7764\u001b[0m  3.0934\n",
      "      8        \u001b[36m0.7159\u001b[0m       \u001b[32m0.7441\u001b[0m        \u001b[35m0.7422\u001b[0m  3.0959\n",
      "      9        \u001b[36m0.6845\u001b[0m       \u001b[32m0.7535\u001b[0m        \u001b[35m0.7172\u001b[0m  3.1002\n",
      "     10        \u001b[36m0.6500\u001b[0m       0.7504        0.7301  3.1135\n",
      "     11        \u001b[36m0.6233\u001b[0m       0.7222        0.8134  3.1064\n",
      "     12        \u001b[36m0.5967\u001b[0m       0.7500        0.7301  3.1062\n",
      "     13        \u001b[36m0.5679\u001b[0m       0.7382        0.7676  3.0977\n",
      "     14        \u001b[36m0.5419\u001b[0m       \u001b[32m0.7573\u001b[0m        \u001b[35m0.7070\u001b[0m  3.1099\n",
      "     15        \u001b[36m0.5189\u001b[0m       0.7275        0.8109  3.1219\n",
      "     16        \u001b[36m0.5012\u001b[0m       0.7383        0.7763  3.1253\n",
      "     17        \u001b[36m0.4856\u001b[0m       \u001b[32m0.7759\u001b[0m        \u001b[35m0.6584\u001b[0m  3.1117\n",
      "     18        \u001b[36m0.4610\u001b[0m       0.7671        0.6920  3.1101\n",
      "     19        \u001b[36m0.4503\u001b[0m       0.7484        0.7565  3.1313\n",
      "     20        \u001b[36m0.4330\u001b[0m       0.7603        0.7146  3.1280\n",
      "     21        \u001b[36m0.4164\u001b[0m       0.7584        0.7378  3.1179\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "The channel was 256, the learning rate was 0.001\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.6579\u001b[0m       \u001b[32m0.5450\u001b[0m        \u001b[35m1.2561\u001b[0m  6.1184\n",
      "      2        \u001b[36m1.1583\u001b[0m       \u001b[32m0.6407\u001b[0m        \u001b[35m1.0060\u001b[0m  6.1430\n",
      "      3        \u001b[36m0.9712\u001b[0m       \u001b[32m0.6851\u001b[0m        \u001b[35m0.9020\u001b[0m  6.1335\n",
      "      4        \u001b[36m0.8689\u001b[0m       \u001b[32m0.7178\u001b[0m        \u001b[35m0.8076\u001b[0m  6.1475\n",
      "      5        \u001b[36m0.7908\u001b[0m       0.7155        0.8156  6.1657\n",
      "      6        \u001b[36m0.7173\u001b[0m       \u001b[32m0.7254\u001b[0m        \u001b[35m0.7869\u001b[0m  6.1685\n",
      "      7        \u001b[36m0.6691\u001b[0m       0.7162        0.8146  6.1811\n",
      "      8        \u001b[36m0.6200\u001b[0m       0.7231        0.8026  6.2037\n",
      "      9        \u001b[36m0.5697\u001b[0m       \u001b[32m0.7593\u001b[0m        \u001b[35m0.6993\u001b[0m  6.1986\n",
      "     10        \u001b[36m0.5247\u001b[0m       0.7511        0.7574  6.2029\n",
      "     11        \u001b[36m0.4784\u001b[0m       \u001b[32m0.7777\u001b[0m        \u001b[35m0.6565\u001b[0m  6.2012\n",
      "     12        \u001b[36m0.4481\u001b[0m       0.7608        0.7093  6.2085\n",
      "     13        \u001b[36m0.4193\u001b[0m       0.7678        0.7088  6.2271\n",
      "     14        \u001b[36m0.3876\u001b[0m       0.7664        0.7174  6.2404\n",
      "     15        \u001b[36m0.3628\u001b[0m       0.7653        0.7162  6.2230\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "The channel was 512, the learning rate was 0.001\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m1.8660\u001b[0m       \u001b[32m0.5239\u001b[0m        \u001b[35m1.3267\u001b[0m  13.8909\n",
      "      2        \u001b[36m1.2144\u001b[0m       \u001b[32m0.6288\u001b[0m        \u001b[35m1.0492\u001b[0m  14.0318\n",
      "      3        \u001b[36m0.9933\u001b[0m       \u001b[32m0.6585\u001b[0m        \u001b[35m1.0033\u001b[0m  14.0499\n",
      "      4        \u001b[36m0.8716\u001b[0m       \u001b[32m0.7019\u001b[0m        \u001b[35m0.8538\u001b[0m  14.0592\n",
      "      5        \u001b[36m0.7827\u001b[0m       \u001b[32m0.7338\u001b[0m        \u001b[35m0.7637\u001b[0m  13.9973\n",
      "      6        \u001b[36m0.7105\u001b[0m       0.7051        0.8631  13.9783\n",
      "      7        \u001b[36m0.6455\u001b[0m       0.7081        0.8460  14.0149\n",
      "      8        \u001b[36m0.5830\u001b[0m       0.7151        0.8477  14.0355\n",
      "      9        \u001b[36m0.5239\u001b[0m       \u001b[32m0.7465\u001b[0m        \u001b[35m0.7621\u001b[0m  14.0760\n",
      "     10        \u001b[36m0.4676\u001b[0m       0.7337        0.8000  14.0717\n",
      "     11        \u001b[36m0.4258\u001b[0m       0.7354        0.8330  13.8616\n",
      "     12        \u001b[36m0.4056\u001b[0m       \u001b[32m0.7492\u001b[0m        0.8092  13.8587\n",
      "     13        \u001b[36m0.3547\u001b[0m       \u001b[32m0.7633\u001b[0m        0.7739  13.9999\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "The channel was 128, the learning rate was 0.01\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m2.4661\u001b[0m       \u001b[32m0.3902\u001b[0m        \u001b[35m1.6711\u001b[0m  3.1283\n",
      "      2        \u001b[36m1.4921\u001b[0m       \u001b[32m0.5048\u001b[0m        \u001b[35m1.3910\u001b[0m  3.1077\n",
      "      3        \u001b[36m1.2807\u001b[0m       \u001b[32m0.5838\u001b[0m        \u001b[35m1.1837\u001b[0m  3.1125\n",
      "      4        \u001b[36m1.1125\u001b[0m       \u001b[32m0.6630\u001b[0m        \u001b[35m0.9687\u001b[0m  3.0927\n",
      "      5        \u001b[36m0.9888\u001b[0m       \u001b[32m0.6790\u001b[0m        \u001b[35m0.9151\u001b[0m  3.0898\n",
      "      6        \u001b[36m0.9001\u001b[0m       0.6382        1.0577  3.0995\n",
      "      7        \u001b[36m0.8429\u001b[0m       0.6774        0.9419  3.0966\n",
      "      8        \u001b[36m0.7930\u001b[0m       \u001b[32m0.7120\u001b[0m        \u001b[35m0.8200\u001b[0m  3.0852\n",
      "      9        \u001b[36m0.7398\u001b[0m       0.6705        0.9908  3.0919\n",
      "     10        \u001b[36m0.7032\u001b[0m       \u001b[32m0.7199\u001b[0m        \u001b[35m0.8102\u001b[0m  3.0905\n",
      "     11        \u001b[36m0.6733\u001b[0m       \u001b[32m0.7302\u001b[0m        \u001b[35m0.7912\u001b[0m  3.0858\n",
      "     12        \u001b[36m0.6391\u001b[0m       0.7149        0.8442  3.0781\n",
      "     13        \u001b[36m0.6147\u001b[0m       0.7170        0.8561  3.0809\n",
      "     14        \u001b[36m0.5882\u001b[0m       0.7029        0.8850  3.0770\n",
      "     15        \u001b[36m0.5654\u001b[0m       0.7282        0.8245  3.0779\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "The channel was 256, the learning rate was 0.01\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m3.5919\u001b[0m       \u001b[32m0.3983\u001b[0m        \u001b[35m1.6982\u001b[0m  6.0360\n",
      "      2        \u001b[36m1.4422\u001b[0m       \u001b[32m0.4942\u001b[0m        \u001b[35m1.4063\u001b[0m  6.0587\n",
      "      3        \u001b[36m1.2272\u001b[0m       \u001b[32m0.6151\u001b[0m        \u001b[35m1.1065\u001b[0m  6.0509\n",
      "      4        \u001b[36m1.0562\u001b[0m       \u001b[32m0.6300\u001b[0m        \u001b[35m1.0532\u001b[0m  6.0635\n",
      "      5        \u001b[36m0.9361\u001b[0m       \u001b[32m0.6477\u001b[0m        \u001b[35m1.0331\u001b[0m  6.0767\n",
      "      6        \u001b[36m0.8409\u001b[0m       0.6235        1.1104  6.0766\n",
      "      7        \u001b[36m0.7679\u001b[0m       \u001b[32m0.7008\u001b[0m        \u001b[35m0.8644\u001b[0m  6.0790\n",
      "      8        \u001b[36m0.6919\u001b[0m       0.6667        0.9843  6.0795\n",
      "      9        \u001b[36m0.6367\u001b[0m       0.6870        0.9418  6.0814\n",
      "     10        \u001b[36m0.5896\u001b[0m       \u001b[32m0.7083\u001b[0m        0.8764  6.0957\n",
      "     11        \u001b[36m0.5484\u001b[0m       \u001b[32m0.7123\u001b[0m        \u001b[35m0.8610\u001b[0m  6.1184\n",
      "     12        \u001b[36m0.4997\u001b[0m       \u001b[32m0.7515\u001b[0m        \u001b[35m0.7285\u001b[0m  6.1226\n",
      "     13        \u001b[36m0.4751\u001b[0m       0.7501        0.7545  6.1230\n",
      "     14        \u001b[36m0.4319\u001b[0m       0.7270        0.8634  6.1406\n",
      "     15        \u001b[36m0.4018\u001b[0m       \u001b[32m0.7519\u001b[0m        0.7678  6.1402\n",
      "     16        \u001b[36m0.3827\u001b[0m       0.7382        0.8527  6.1434\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "The channel was 512, the learning rate was 0.01\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m6.0569\u001b[0m       \u001b[32m0.4086\u001b[0m        \u001b[35m1.6634\u001b[0m  13.7351\n",
      "      2        \u001b[36m1.5010\u001b[0m       \u001b[32m0.4979\u001b[0m        \u001b[35m1.4120\u001b[0m  13.8113\n",
      "      3        \u001b[36m1.2651\u001b[0m       \u001b[32m0.5863\u001b[0m        \u001b[35m1.1882\u001b[0m  13.7868\n",
      "      4        \u001b[36m1.0987\u001b[0m       \u001b[32m0.6096\u001b[0m        \u001b[35m1.1351\u001b[0m  13.8378\n",
      "      5        \u001b[36m0.9611\u001b[0m       \u001b[32m0.6584\u001b[0m        \u001b[35m0.9870\u001b[0m  13.8555\n",
      "      6        \u001b[36m0.8590\u001b[0m       \u001b[32m0.6933\u001b[0m        \u001b[35m0.8747\u001b[0m  13.9215\n",
      "      7        \u001b[36m0.7620\u001b[0m       \u001b[32m0.7215\u001b[0m        \u001b[35m0.8052\u001b[0m  13.9019\n",
      "      8        \u001b[36m0.6876\u001b[0m       \u001b[32m0.7403\u001b[0m        \u001b[35m0.7532\u001b[0m  13.9872\n",
      "      9        \u001b[36m0.6161\u001b[0m       \u001b[32m0.7516\u001b[0m        \u001b[35m0.7395\u001b[0m  13.9732\n",
      "     10        \u001b[36m0.5579\u001b[0m       0.6949        0.9249  13.9310\n",
      "     11        \u001b[36m0.4903\u001b[0m       0.6445        1.1340  13.9594\n",
      "     12        \u001b[36m0.4353\u001b[0m       0.7367        0.7914  13.9665\n",
      "     13        \u001b[36m0.3780\u001b[0m       \u001b[32m0.7526\u001b[0m        0.7753  13.9434\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n"
     ]
    }
   ],
   "source": [
    "# implement hyperparameters, you can select and modify the hyperparameters by yourself here.\n",
    "learning_rate = [1e-4, 1e-3, 1e-2]\n",
    "channel = [128, 256, 512]\n",
    "# Select the better optimizer by the result shown in the previous problem, you can select and modify it by yourself here.\n",
    "\n",
    "better_optimizer = torch.optim.Adam  \n",
    "\n",
    "train_data_normalized = torch.Tensor(train.data/255)\n",
    "train_data_normalized = train_data_normalized.permute(0,3,1,2)\n",
    "\n",
    "for l in learning_rate:\n",
    "    for c in channel:\n",
    "      print(f'The channel was {c}, the learning rate was {l}')\n",
    "\n",
    "      cnn = CNN_MaxPool(channels = c)\n",
    "      \n",
    "      model = skorch.NeuralNetClassifier(cnn, criterion=torch.nn.CrossEntropyLoss,\n",
    "                                   device=\"cuda\",\n",
    "                                   optimizer=better_optimizer,\n",
    "                                   lr=l,\n",
    "                                   max_epochs=50,\n",
    "                                   batch_size=32 * 8,\n",
    "                                   callbacks=[skorch.callbacks.EarlyStopping(lower_is_better=True)])\n",
    "      # implement input normalization & type cast here \n",
    "      model.fit(train_data_normalized, np.asarray(train.targets))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H7Mu2ZZHoZU0"
   },
   "source": [
    "Write down the **validation accuracy** of the model under different hyperparameter settings.\n",
    "\n",
    "learning rate = $10^{-4}$:\n",
    "| #channel for each layer | validation accuracy |\n",
    "|:-----------------------:|:-------------------:|\n",
    "|          $128$          |      $0.7755$       |\n",
    "|          $256$          |    **$0.7860$**     |\n",
    "|          $512$          |      $0.7801$       |\n",
    "\n",
    "learning rate = $10^{-3}$:\n",
    "| #channel for each layer | validation accuracy |\n",
    "|:-----------------------:|:-------------------:|\n",
    "|          $128$          |      $0.7759$       |\n",
    "|          $256$          |      $0.7777$       |\n",
    "|          $512$          |      $0.7633$       |\n",
    "\n",
    "learning rate = $10^{-2}$:\n",
    "| #channel for each layer | validation accuracy |\n",
    "|:-----------------------:|:-------------------:|\n",
    "|          $128$          |      $0.7302$       |\n",
    "|          $256$          |      $0.7519$       |\n",
    "|          $512$          |      $0.7526$       |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8UCaz8nWoWWS"
   },
   "source": [
    "For the best model you have, test it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "bTtBk22OECDD",
    "outputId": "40229a37-f53c-4837-de75-8a63b3f66889"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.6229\u001b[0m       \u001b[32m0.5552\u001b[0m        \u001b[35m1.2808\u001b[0m  5.7887\n",
      "      2        \u001b[36m1.2905\u001b[0m       \u001b[32m0.6147\u001b[0m        \u001b[35m1.1130\u001b[0m  5.7904\n",
      "      3        \u001b[36m1.1429\u001b[0m       \u001b[32m0.6455\u001b[0m        \u001b[35m1.0174\u001b[0m  5.7758\n",
      "      4        \u001b[36m1.0483\u001b[0m       \u001b[32m0.6692\u001b[0m        \u001b[35m0.9528\u001b[0m  5.7884\n",
      "      5        \u001b[36m0.9770\u001b[0m       \u001b[32m0.6930\u001b[0m        \u001b[35m0.8981\u001b[0m  5.7653\n",
      "      6        \u001b[36m0.9212\u001b[0m       \u001b[32m0.7024\u001b[0m        \u001b[35m0.8618\u001b[0m  5.7668\n",
      "      7        \u001b[36m0.8761\u001b[0m       \u001b[32m0.7114\u001b[0m        \u001b[35m0.8418\u001b[0m  5.8012\n",
      "      8        \u001b[36m0.8357\u001b[0m       \u001b[32m0.7177\u001b[0m        \u001b[35m0.8171\u001b[0m  5.7866\n",
      "      9        \u001b[36m0.8008\u001b[0m       \u001b[32m0.7259\u001b[0m        \u001b[35m0.7928\u001b[0m  5.7409\n",
      "     10        \u001b[36m0.7699\u001b[0m       \u001b[32m0.7294\u001b[0m        \u001b[35m0.7760\u001b[0m  5.7591\n",
      "     11        \u001b[36m0.7416\u001b[0m       \u001b[32m0.7389\u001b[0m        \u001b[35m0.7623\u001b[0m  5.7847\n",
      "     12        \u001b[36m0.7144\u001b[0m       \u001b[32m0.7436\u001b[0m        \u001b[35m0.7479\u001b[0m  5.7757\n",
      "     13        \u001b[36m0.6867\u001b[0m       \u001b[32m0.7459\u001b[0m        \u001b[35m0.7379\u001b[0m  5.7796\n",
      "     14        \u001b[36m0.6621\u001b[0m       \u001b[32m0.7522\u001b[0m        \u001b[35m0.7261\u001b[0m  5.7849\n",
      "     15        \u001b[36m0.6387\u001b[0m       \u001b[32m0.7562\u001b[0m        \u001b[35m0.7117\u001b[0m  5.7848\n",
      "     16        \u001b[36m0.6178\u001b[0m       \u001b[32m0.7578\u001b[0m        \u001b[35m0.7046\u001b[0m  5.7691\n",
      "     17        \u001b[36m0.5950\u001b[0m       \u001b[32m0.7620\u001b[0m        \u001b[35m0.6940\u001b[0m  5.7612\n",
      "     18        \u001b[36m0.5779\u001b[0m       \u001b[32m0.7624\u001b[0m        \u001b[35m0.6915\u001b[0m  5.7960\n",
      "     19        \u001b[36m0.5589\u001b[0m       \u001b[32m0.7671\u001b[0m        \u001b[35m0.6826\u001b[0m  5.7776\n",
      "     20        \u001b[36m0.5397\u001b[0m       \u001b[32m0.7683\u001b[0m        \u001b[35m0.6823\u001b[0m  5.7812\n",
      "     21        \u001b[36m0.5170\u001b[0m       \u001b[32m0.7703\u001b[0m        \u001b[35m0.6769\u001b[0m  5.7648\n",
      "     22        \u001b[36m0.5039\u001b[0m       \u001b[32m0.7711\u001b[0m        \u001b[35m0.6676\u001b[0m  5.7785\n",
      "     23        \u001b[36m0.4839\u001b[0m       \u001b[32m0.7740\u001b[0m        \u001b[35m0.6576\u001b[0m  5.7657\n",
      "     24        \u001b[36m0.4651\u001b[0m       \u001b[32m0.7744\u001b[0m        \u001b[35m0.6569\u001b[0m  5.7748\n",
      "     25        \u001b[36m0.4493\u001b[0m       \u001b[32m0.7770\u001b[0m        \u001b[35m0.6514\u001b[0m  5.7814\n",
      "     26        \u001b[36m0.4353\u001b[0m       \u001b[32m0.7772\u001b[0m        0.6568  5.7899\n",
      "     27        \u001b[36m0.4183\u001b[0m       \u001b[32m0.7785\u001b[0m        0.6526  5.7753\n",
      "     28        \u001b[36m0.4051\u001b[0m       \u001b[32m0.7792\u001b[0m        \u001b[35m0.6506\u001b[0m  5.7794\n",
      "     29        \u001b[36m0.3886\u001b[0m       \u001b[32m0.7831\u001b[0m        \u001b[35m0.6463\u001b[0m  5.7860\n",
      "     30        \u001b[36m0.3738\u001b[0m       0.7800        \u001b[35m0.6461\u001b[0m  5.7802\n",
      "     31        \u001b[36m0.3578\u001b[0m       \u001b[32m0.7841\u001b[0m        \u001b[35m0.6455\u001b[0m  5.7607\n",
      "     32        \u001b[36m0.3497\u001b[0m       0.7830        \u001b[35m0.6408\u001b[0m  5.7976\n",
      "     33        \u001b[36m0.3381\u001b[0m       0.7836        \u001b[35m0.6397\u001b[0m  5.7657\n",
      "     34        \u001b[36m0.3251\u001b[0m       0.7826        0.6426  5.7709\n",
      "     35        \u001b[36m0.3099\u001b[0m       \u001b[32m0.7844\u001b[0m        0.6464  5.7790\n",
      "     36        \u001b[36m0.2998\u001b[0m       \u001b[32m0.7850\u001b[0m        0.6435  5.7733\n",
      "     37        \u001b[36m0.2838\u001b[0m       \u001b[32m0.7862\u001b[0m        \u001b[35m0.6367\u001b[0m  5.7886\n",
      "     38        \u001b[36m0.2772\u001b[0m       0.7858        0.6401  5.7790\n",
      "     39        \u001b[36m0.2692\u001b[0m       \u001b[32m0.7871\u001b[0m        0.6405  5.8318\n",
      "     40        \u001b[36m0.2595\u001b[0m       0.7866        \u001b[35m0.6365\u001b[0m  5.7907\n",
      "     41        \u001b[36m0.2484\u001b[0m       \u001b[32m0.7872\u001b[0m        0.6409  5.7713\n",
      "     42        \u001b[36m0.2385\u001b[0m       0.7827        0.6520  5.7901\n",
      "     43        \u001b[36m0.2300\u001b[0m       0.7860        0.6646  5.7787\n",
      "     44        \u001b[36m0.2202\u001b[0m       0.7845        0.6562  5.7917\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7804"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implement the same input normalization & type cast here\n",
    "\n",
    "optimizer = torch.optim.Adam \n",
    "learning_rate = 1e-4\n",
    "channel = 256\n",
    "\n",
    "cnn = CNN_MaxPool(channels = channel)\n",
    "model = skorch.NeuralNetClassifier(cnn, criterion=torch.nn.CrossEntropyLoss,\n",
    "                                   device=\"cuda\",\n",
    "                                   optimizer=optimizer,\n",
    "                                   lr=learning_rate,\n",
    "                                   max_epochs=50,\n",
    "                                   batch_size=32 * 8,\n",
    "                                   callbacks=[skorch.callbacks.EarlyStopping(lower_is_better=True)])\n",
    "\n",
    "# implement input normalization & type cast here \n",
    "model.fit(train_data_normalized, np.asarray(train.targets))\n",
    "\n",
    "\n",
    "test_data_normalized = torch.Tensor(test.data/255) \n",
    "test_data_normalized = test_data_normalized.permute(0,3,1,2)\n",
    "test.predictions = model.predict(test_data_normalized)\n",
    "sklearn.metrics.accuracy_score(test.targets, test.predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kSbhC8f1or6_"
   },
   "source": [
    "How much **test accuracy** do you get? What can you conclude for the design of CNN structure and tuning of hyperparameters? (5 points)\n",
    "\n",
    "**Your Answer:** \n",
    "\n",
    "1. The test accuracy we got is $0.7804$.$\\\\$\n",
    "    The best model is the one with $256$ channels for each layer and learning rate $10^{-4}$.\n",
    "\n",
    "\n",
    "2. From the results above, we can conclude that the number of channels, learning rate and the optimizer are important hyperparameters, they could greatly influence the training speed and evaluation effects. Also the structure of CNN is also important, the max pooling layer could improve the performance of the model. And with dropout layer, the model could be more robust to against overfitting.$\\\\$\n",
    "The number of channels which also influence the net structure, could effect performence as hyperparameter. It should not be too small or too large. The learning rate should not be too large, otherwise the model will not converge."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "Adrien Hernandez Homework 1 - PyTorch",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0f3b4a3b037d47cb8ba1196d7dbd3b98": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "310efd92386a4757a767d5204b08a865": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_87cad6ba7ef84d03a7f8c0cb37ec01ec",
       "IPY_MODEL_b712d5d1ba464a378c00dbb2aec6dd8c"
      ],
      "layout": "IPY_MODEL_8346dadd8dcd437f9d0c73a8e86d4503"
     }
    },
    "3f3ef2c146bf4dd38424170b3090b055": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "44a89539615b4132a01ca7f5223ca281": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8346dadd8dcd437f9d0c73a8e86d4503": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "87cad6ba7ef84d03a7f8c0cb37ec01ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f3ef2c146bf4dd38424170b3090b055",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0f3b4a3b037d47cb8ba1196d7dbd3b98",
      "value": 1
     }
    },
    "adbb9a799ba44a91b927b44d0c27e6d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b712d5d1ba464a378c00dbb2aec6dd8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_44a89539615b4132a01ca7f5223ca281",
      "placeholder": "​",
      "style": "IPY_MODEL_adbb9a799ba44a91b927b44d0c27e6d1",
      "value": "170500096it [00:04, 37859309.77it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
